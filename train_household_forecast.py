#å¾€æ·»åŠ ç¯å¢ƒç‰¹å¾æ–¹å‘èµ°
"""
å®¶åº­è´Ÿè·é¢„æµ‹è„šæœ¬ï¼ˆCNNã€LSTMã€GRUï¼‰
è¯´æ˜ï¼š
- ä»æ–‡ä»¶å¤¹è¯»å–æŸæˆ·è´Ÿè·CSVï¼Œé»˜è®¤ä½¿ç”¨æ–‡ä»¶å¤¹ä¸­ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶ã€‚
- å°†åŸå§‹æ—¶é—´åºåˆ—èšåˆä¸ºæ—¥å°ºåº¦ç‰¹å¾ï¼šå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€æ–œç‡ã€‚
- ä½¿ç”¨å‰3ä¸ªæœˆï¼ˆçº¦90å¤©ï¼‰æ ·æœ¬è®­ç»ƒï¼Œå3ä¸ªæœˆæ ·æœ¬éªŒè¯ï¼›è‹¥æ•°æ®ä¸è¶³åˆ™é€€åŒ–ä¸º70/30åˆ†å‰²ã€‚
- ä½¿ç”¨æ»‘åŠ¨çª—å£æ„å»ºåºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€æ—¥çš„æ—¥å‡è´Ÿè·ï¼ˆå›å½’ï¼‰ã€‚
- è¾“å‡ºæ¯ä¸ªæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„MAPEå’ŒRMSEã€‚
"""

import os
import glob
import numpy as np
import pandas as pd
from datetime import timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras import layers, models
from scipy.ndimage import gaussian_filter1d
from scipy.stats import kurtosis
import math
from scipy.stats import norm
import matplotlib.pyplot as plt
import re
import random
import warnings
from sklearn.mixture import GaussianMixture

# å°è¯•å¯¼å…¥hmmlearnï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
try:
    from hmmlearn import hmm
    HMM_AVAILABLE = True
except ImportError:
    HMM_AVAILABLE = False
    print("âš ï¸ hmmlearn æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•åˆ†æ®µæ–¹æ³•ä½œä¸ºæ›¿ä»£")

# æŠ‘åˆ¶ç‰¹å®šçš„RuntimeWarning
warnings.filterwarnings('ignore', message='Precision loss occurred in moment calculation*')
# æˆ–è€…æŠ‘åˆ¶æ‰€æœ‰scipyç»Ÿè®¡ç›¸å…³è­¦å‘Š
warnings.filterwarnings('ignore', category=RuntimeWarning, module='scipy.stats')
# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
# æ·»åŠ ä¸­æ–‡å­—ä½“é…ç½®
import matplotlib
from matplotlib import font_manager

# é…ç½®
DATA_FOLDER = r"D:\python project\xuheng-homeloadprediction\OutputData\combined\all"
MODEL_OUTPUT_DIR = r"D:\python project\è´Ÿè·é¢„æµ‹é¡¹ç›®\output\model"
ANALYSIS_OUTPUT_DIR = r"D:\python project\è´Ÿè·é¢„æµ‹é¡¹ç›®\output\analysis"
SEQ_LEN = 14  # ç”¨è¿‡å»14å¤©é¢„æµ‹ä¸‹ä¸€å¤©
EPOCHS = 50
BATCH_SIZE = 64
RANDOM_SEED = 42

# è‹¥éœ€æŒ‡å®šç»˜å›¾çš„æ—¥æœŸï¼ˆæ ¼å¼ 'YYYY-MM-DD'ï¼‰ï¼Œå¯è®¾ç½® PLOT_DATEï¼›é»˜è®¤ä¸º Noneï¼ˆä½¿ç”¨éªŒè¯é›†ä¸­çš„ç¬¬ä¸€æ¡æ ·æœ¬æ—¥æœŸï¼‰
PLOT_DATE = None

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡å­—ä½“
HAS_CJK_FONT = True
try:
    # å°è¯•ä½¿ç”¨ä¸­æ–‡å­—ä½“
    test_font = font_manager.FontProperties(family='SimHei')
    if not test_font:
        HAS_CJK_FONT = False
except:
    HAS_CJK_FONT = False

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)
os.makedirs(ANALYSIS_OUTPUT_DIR, exist_ok=True)


def find_csv_file(folder):
    files = glob.glob(os.path.join(folder, "**/*.csv"), recursive=True)
    if not files:
        raise FileNotFoundError(f"åœ¨ {folder} æœªæ‰¾åˆ° CSV æ–‡ä»¶")
    files.sort()
    return files[0]

# æ–°å¢ï¼šè¿”å›æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ CSV åˆ—è¡¨ï¼ˆæŒ‰è·¯å¾„æ’åºï¼‰
def find_csv_files(folder):
    files = glob.glob(os.path.join(folder, "**/*.csv"), recursive=True)
    if not files:
        raise FileNotFoundError(f"åœ¨ {folder} æœªæ‰¾åˆ° CSV æ–‡ä»¶")
    files.sort()
    return files

def find_apartment_files(folder):
    """æŸ¥æ‰¾æ‰€æœ‰å…¬å¯“æ•°æ®æ–‡ä»¶ï¼ˆApt*_2015.csvæ ¼å¼ï¼‰å¹¶æŒ‰æˆ·å·æ’åº"""
    pattern = os.path.join(folder, "Apt*_2015.csv")
    files = glob.glob(pattern)
    if not files:
        raise FileNotFoundError(f"åœ¨ {folder} æœªæ‰¾åˆ° Apt*_2015.csv æ ¼å¼çš„æ–‡ä»¶")
    
    # æŒ‰æˆ·å·æ’åº
    def extract_apt_number(filename):
        import re
        basename = os.path.basename(filename)
        match = re.search(r'Apt(\d+)_2015\.csv', basename)
        return int(match.group(1)) if match else 0
    
    files.sort(key=extract_apt_number)
    return files

def extract_household_name(csv_path):
    """ä»CSVæ–‡ä»¶è·¯å¾„æå–å®¶åº­åç§°"""
    basename = os.path.basename(csv_path)
    # å»æ‰æ–‡ä»¶æ‰©å±•å
    name = os.path.splitext(basename)[0]
    return name

def interactive_select_households():
    """äº¤äº’å¼é€‰æ‹©è¦è®­ç»ƒçš„æˆ·æ•°"""
    print("\nğŸ  å®¶åº­è´Ÿè·é¢„æµ‹è®­ç»ƒ - å•æˆ·é€‰æ‹©")
    print("="*60)
    
    # æŸ¥æ‰¾æ‰€æœ‰å…¬å¯“æ–‡ä»¶
    try:
        all_files = find_apartment_files(DATA_FOLDER)
        print(f"ğŸ“Š å‘ç° {len(all_files)} æˆ·æ•°æ®æ–‡ä»¶")
        
        # æ˜¾ç¤ºå‰10ä¸ªå’Œå10ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
        print("\nğŸ“‹ å¯ç”¨æˆ·æ•°æ®æ–‡ä»¶ç¤ºä¾‹ï¼š")
        if len(all_files) <= 20:
            for i, file in enumerate(all_files, 1):
                basename = os.path.basename(file)
                print(f"  {i:3d}. {basename}")
        else:
            for i in range(10):
                basename = os.path.basename(all_files[i])
                print(f"  {i+1:3d}. {basename}")
            print("  ...")
            for i in range(len(all_files)-10, len(all_files)):
                basename = os.path.basename(all_files[i])
                print(f"  {i+1:3d}. {basename}")
        
        print(f"\nğŸ¯ è¯·é€‰æ‹©è¦è®­ç»ƒçš„ç”¨æˆ·ï¼š")
        print("è¾“å…¥æ–¹å¼ï¼š")
        print("  - æˆ·å·ï¼šç›´æ¥è¾“å…¥æˆ·å·ï¼ˆå¦‚ 1, 2, 1114ï¼‰")
        print("  - åºå·ï¼šè¾“å…¥æ–‡ä»¶åºå·ï¼ˆå¦‚ 1-114ï¼Œå¯¹åº”ä¸Šé¢åˆ—è¡¨ä¸­çš„åºå·ï¼‰")
        
        while True:
            try:
                selection = input("\nè¯·è¾“å…¥æˆ·å·æˆ–æ–‡ä»¶åºå·: ").strip()
                
                if not selection.isdigit():
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                    continue
                
                num = int(selection)
                selected_files = []
                
                if num <= len(all_files):
                    # ä½œä¸ºåºå·å¤„ç†
                    selected_files = [all_files[num-1]]
                    print(f"âœ… æŒ‰æ–‡ä»¶åºå·é€‰æ‹©: {os.path.basename(selected_files[0])}")
                else:
                    # ä½œä¸ºæˆ·å·å¤„ç†ï¼ŒæŸ¥æ‰¾å¯¹åº”æ–‡ä»¶
                    target_file = None
                    for file in all_files:
                        if f"Apt{num}_2015.csv" in os.path.basename(file):
                            target_file = file
                            break
                    if target_file:
                        selected_files = [target_file]
                        print(f"âœ… æŒ‰æˆ·å·é€‰æ‹©: {os.path.basename(selected_files[0])}")
                    else:
                        print(f"âŒ æœªæ‰¾åˆ°æˆ·å· {num} çš„æ•°æ®æ–‡ä»¶")
                        continue
                
                # ç¡®è®¤é€‰æ‹©å¹¶è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°
                confirm = input(f"\nç¡®è®¤ä½¿ç”¨ {os.path.basename(selected_files[0])} è¿›è¡Œè®­ç»ƒï¼Ÿ(y/n): ").strip().lower()
                if confirm in ['y', 'yes', 'æ˜¯', '']:
                    # è®©ç”¨æˆ·è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°
                    while True:
                        model_name = input(f"\nè¯·è¾“å…¥æ¨¡å‹åç§°ï¼ˆç”¨äºä¿å­˜æ¨¡å‹æ–‡ä»¶ï¼‰: ").strip()
                        if not model_name:
                            print("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
                            continue
                        # æ£€æŸ¥åç§°æ˜¯å¦åˆæ³•ï¼ˆé¿å…ç‰¹æ®Šå­—ç¬¦ï¼‰
                        import re
                        if not re.match(r'^[a-zA-Z0-9_\-\u4e00-\u9fa5]+$', model_name):
                            print("âŒ æ¨¡å‹åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦å’Œä¸­æ–‡å­—ç¬¦")
                            continue
                        break
                    return selected_files, 'single', model_name
                else:
                    print("è¯·é‡æ–°é€‰æ‹©...")
                    continue
                        
            except ValueError:
                print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—")
                continue
            except KeyboardInterrupt:
                print("\n\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return [], 'cancel', None
            except Exception as e:
                print(f"âŒ é€‰æ‹©è¿‡ç¨‹å‡ºé”™: {e}")
                continue
                
    except Exception as e:
        print(f"âŒ æŸ¥æ‰¾æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return [], 'error', None


def load_time_series(csv_path):
    # å°è¯•è¯»å–å¹¶è§£ææ—¶é—´åˆ—ã€è´Ÿè·åˆ—å’Œç¯å¢ƒç‰¹å¾åˆ—
    df = pd.read_csv(csv_path, encoding='utf-8')

    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶åˆ—å: {df.columns.tolist()}")

    # è¯†åˆ«æ—¶é—´åˆ—
    time_cols = [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower() or 'datetime' in c.lower()]
    if time_cols:
        tcol = time_cols[0]
    else:
        tcol = df.columns[0]

    # è¯†åˆ«è´Ÿè·åˆ—ï¼ˆé€šå¸¸æ˜¯Valueåˆ—ï¼‰
    load_col = None
    if 'Value' in df.columns:
        load_col = 'Value'
    elif 'value' in df.columns:
        load_col = 'value'
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¯»æ‰¾æ•°å€¼åˆ—
        num_cols = [c for c in df.columns if c != tcol and np.issubdtype(df[c].dtype, np.number)]
        if num_cols:
            load_col = num_cols[0]
        else:
            raise ValueError('æ— æ³•è¯†åˆ«è´Ÿè·æ•°å€¼åˆ—')

    # å®šä¹‰éœ€è¦çš„ç¯å¢ƒç‰¹å¾åˆ—
    env_features = ['temperature', 'humidity', 'visibility', 'pressure', 'windSpeed', 'cloudCover', 'dewPoint']
    
    # æ£€æŸ¥å“ªäº›ç¯å¢ƒç‰¹å¾åˆ—å­˜åœ¨
    available_env_features = []
    for feat in env_features:
        if feat in df.columns:
            available_env_features.append(feat)
    
    print(f"ğŸ“Š å‘ç°ç¯å¢ƒç‰¹å¾: {available_env_features}")

    # é€‰æ‹©éœ€è¦çš„åˆ—
    selected_cols = [tcol, load_col] + available_env_features
    
    # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
    missing_cols = [col for col in selected_cols if col not in df.columns]
    if missing_cols:
        print(f"âš ï¸ ç¼ºå°‘åˆ—: {missing_cols}")
        # ç§»é™¤ç¼ºå¤±çš„åˆ—
        selected_cols = [col for col in selected_cols if col in df.columns]

    # è½¬æ¢æ•°æ®ç±»å‹
    for col in selected_cols[1:]:  # è·³è¿‡æ—¶é—´åˆ—
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # è§£ææ—¶é—´å¹¶è®¾ç½®ç´¢å¼•
    try:
        df[tcol] = pd.to_datetime(df[tcol])
    except Exception:
        # è‹¥æ— æ³•è§£ææ—¶é—´ï¼Œåˆ™æŒ‰è¡Œå·ç”Ÿæˆæ—¶é—´ï¼ˆæ¯15åˆ†é’Ÿï¼‰
        df[tcol] = pd.date_range(start='2020-01-01', periods=len(df), freq='15T')

    # åªä¿ç•™é€‰æ‹©çš„åˆ—
    df = df[selected_cols].copy()
    
    # æ¸©åº¦è½¬æ¢ï¼šåæ°åº¦è½¬æ‘„æ°åº¦
    if 'temperature' in df.columns:
        df['temperature'] = (df['temperature'] - 32) * 5/9
        print("ğŸŒ¡ï¸ å·²å°†æ¸©åº¦ä»åæ°åº¦è½¬æ¢ä¸ºæ‘„æ°åº¦")

    # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    df = df.dropna(subset=[load_col])  # ç§»é™¤è´Ÿè·æ•°æ®ç¼ºå¤±çš„è¡Œ
    
    # å¯¹ç¯å¢ƒç‰¹å¾çš„ç¼ºå¤±å€¼è¿›è¡Œæ’å€¼
    for feat in available_env_features:
        if df[feat].isna().any():
            df[feat] = df[feat].interpolate(method='time').fillna(method='ffill').fillna(method='bfill')

    df = df.sort_values(tcol).set_index(tcol)

    # é‡å‘½åè´Ÿè·åˆ—
    rename_dict = {load_col: 'load'}
    df = df.rename(columns=rename_dict)

    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼ŒåŒ…å« {len(df)} è¡Œï¼Œ{len(df.columns)} ä¸ªç‰¹å¾")
    print(f"ğŸ“Š ç‰¹å¾åˆ—: {df.columns.tolist()}")

    return df

1
def smooth_series(series, sigma=2.0):
    arr = series.to_numpy().astype(float)
    return gaussian_filter1d(arr, sigma=sigma)


def build_time_features(df, sigma=2.0, seq_len=96):
    """
    å¯¹åŸå§‹æŒ‰æ—¶é—´åºåˆ—çš„è´Ÿè·æ•°æ®åšå¹³æ»‘å¹¶æå–ç‰¹å¾ï¼ŒåŒæ—¶åŒ…å«ç¯å¢ƒç‰¹å¾ã€‚
    å‡è®¾åŸå§‹æ—¶é—´é—´éš”ä¸º15åˆ†é’Ÿï¼ˆæ¯æ—¥96ä¸ªç‚¹ï¼‰ï¼Œè‹¥ä¸åŒè¯·è°ƒæ•´lagsã€‚
    è¿”å›ç‰¹å¾DataFrameï¼ˆä¸åŸå§‹ç´¢å¼•å¯¹é½ï¼Œä¸å†è¿‡æ—©ä¸¢å¼ƒå‰éƒ¨æ ·æœ¬ï¼›ç‰¹å¾ä¸è¶³å¤„ç½®ä¸º NaNï¼Œåç»­ç»Ÿä¸€å¡«å……ï¼‰ã€‚
    """
    arr = df['load'].astype(float)
    smooth = pd.Series(smooth_series(arr, sigma=sigma), index=df.index)

    # æ£€æŸ¥å“ªäº›ç¯å¢ƒç‰¹å¾å¯ç”¨
    env_features = ['temperature', 'humidity', 'visibility', 'pressure', 'windSpeed', 'cloudCover', 'dewPoint']
    available_env_features = [feat for feat in env_features if feat in df.columns]
    
    print(f"ğŸ“Š æ„å»ºç‰¹å¾æ—¶ä½¿ç”¨çš„ç¯å¢ƒç‰¹å¾: {available_env_features}")

    n = len(smooth)
    # å®šä¹‰æ»åæ­¥é•¿ï¼ˆ15minåˆ†è¾¨ç‡ï¼‰
    lag_1d = 96
    lag_7d = lag_1d * 7

    records = []
    idxs = []

    for i in range(n):
        cur_idx = df.index[i]
        cur_val = smooth.iloc[i]

        feat = {}
        # åŸå§‹ä¸å¹³æ»‘å€¼
        feat['load_smooth'] = float(cur_val)
        feat['load_raw'] = float(arr.iloc[i])

        # æ»åç‰¹å¾ï¼ˆå¦‚æœå†å²ä¸è¶³åˆ™ä¸º NaNï¼‰
        feat['lag_1d'] = float(smooth.iloc[i - lag_1d]) if i - lag_1d >= 0 else np.nan
        feat['lag_7d'] = float(smooth.iloc[i - lag_7d]) if i - lag_7d >= 0 else np.nan

        # è¿‡å»7å¤©åŒæœŸä¸­ä½æ•°ï¼ˆå–æœ€è¿‘7ä¸ªç›¸åŒæ—¶é—´ç‚¹ï¼‰
        same_times = []
        for d in range(1, 8):
            idx = i - d * lag_1d
            if idx >= 0:
                same_times.append(smooth.iloc[idx])
        feat['median_past7_same_time'] = float(np.median(same_times)) if same_times else np.nan

        # è¿‡å»1å°æ—¶ï¼ˆ4ä¸ªç‚¹ï¼‰ä¸1å¤©ï¼ˆ96ä¸ªç‚¹ï¼‰çš„ç»Ÿè®¡ç‰¹å¾
        h1_start = max(0, i - 4)
        h1_window = smooth.iloc[h1_start:i] if i > 0 else smooth.iloc[0:0]
        d1_start = max(0, i - lag_1d)
        d1_window = smooth.iloc[d1_start:i] if i > 0 else smooth.iloc[0:0]

        feat['h1_mean'] = float(h1_window.mean()) if len(h1_window) > 0 else np.nan
        feat['h1_var'] = float(h1_window.var()) if len(h1_window) > 0 else np.nan
        feat['h1_kurtosis'] = float(kurtosis(h1_window)) if len(h1_window) > 0 else np.nan

        feat['d1_mean'] = float(d1_window.mean()) if len(d1_window) > 0 else np.nan
        feat['d1_var'] = float(d1_window.var()) if len(d1_window) > 0 else np.nan
        feat['d1_kurtosis'] = float(kurtosis(d1_window)) if len(d1_window) > 0 else np.nan

        # æœ€å¤§è´Ÿè½½ç‡ï¼ˆçª—å£æœ€å¤§/çª—å£å¹³å‡ï¼‰
        feat['h1_max_load_rate'] = float(h1_window.max() / (h1_window.mean() + 1e-8)) if len(h1_window) > 0 else np.nan
        feat['d1_max_load_rate'] = float(d1_window.max() / (d1_window.mean() + 1e-8)) if len(d1_window) > 0 else np.nan

        # é¢‘åŸŸç‰¹å¾ - å¯¹è¿‡å»ä¸€å¤©çª—å£åšFFTï¼Œå–å‰å‡ ä¸ªå¹…å€¼
        fft_window = d1_window if len(d1_window) >= 8 else smooth.iloc[max(0, i - 32):i]
        if len(fft_window) > 3:
            fft_vals = np.abs(np.fft.rfft(fft_window.values - np.mean(fft_window.values)))
            # å–å‰3ä¸ªé¢‘ç‡å¹…å€¼ï¼ˆé™¤ç›´æµï¼‰
            fft_vals = fft_vals[1:4] if len(fft_vals) > 3 else np.pad(fft_vals[1:], (0, max(0, 3 - len(fft_vals[1:]))), 'constant')
            # å¯èƒ½å› é•¿åº¦é—®é¢˜å¯¼è‡´å•å…ƒç´ æ•°ç»„ï¼Œéœ€è¦å®‰å…¨ç´¢å¼•
            fft_padded = np.pad(fft_vals, (0, 3 - len(fft_vals)), 'constant')
            feat['fft_1'] = float(fft_padded[0])
            feat['fft_2'] = float(fft_padded[1])
            feat['fft_3'] = float(fft_padded[2])
        else:
            feat['fft_1'] = feat['fft_2'] = feat['fft_3'] = 0.0

        # æ—¶é—´ç›¸å…³ç‰¹å¾ï¼ˆå°æ—¶ã€åˆ†é’Ÿï¼‰
        feat['hour'] = float(cur_idx.hour)
        feat['minute'] = float(cur_idx.minute)

        # æ·»åŠ ç¯å¢ƒç‰¹å¾
        for env_feat in available_env_features:
            # å½“å‰æ—¶åˆ»çš„ç¯å¢ƒç‰¹å¾
            feat[f'{env_feat}_current'] = float(df[env_feat].iloc[i])
            
            # è¿‡å»1å°æ—¶å¹³å‡å€¼
            h1_env_start = max(0, i - 4)
            h1_env_window = df[env_feat].iloc[h1_env_start:i] if i > 0 else df[env_feat].iloc[0:0]
            feat[f'{env_feat}_h1_mean'] = float(h1_env_window.mean()) if len(h1_env_window) > 0 else feat[f'{env_feat}_current']
            
            # è¿‡å»1å¤©å¹³å‡å€¼
            d1_env_start = max(0, i - lag_1d)
            d1_env_window = df[env_feat].iloc[d1_env_start:i] if i > 0 else df[env_feat].iloc[0:0]
            feat[f'{env_feat}_d1_mean'] = float(d1_env_window.mean()) if len(d1_env_window) > 0 else feat[f'{env_feat}_current']
            
            # æ»åç‰¹å¾ï¼ˆ1å¤©å‰ï¼‰
            feat[f'{env_feat}_lag_1d'] = float(df[env_feat].iloc[i - lag_1d]) if i - lag_1d >= 0 else feat[f'{env_feat}_current']

        # ç›®æ ‡å€¼ï¼ˆä¸‹ä¸€æ—¶åˆ»çš„å¹³æ»‘è´Ÿè·ï¼‰ï¼Œç”¨äºç›‘ç£å­¦ä¹ 
        if i + 1 < n:
            feat['target_next'] = float(smooth.iloc[i + 1])
        else:
            feat['target_next'] = np.nan

        records.append(feat)
        idxs.append(cur_idx)

    feat_df = pd.DataFrame(records, index=pd.DatetimeIndex(idxs))
    # ä¸¢å¼ƒåŒ…å«NaNç›®æ ‡çš„å°¾éƒ¨ï¼ˆæœ€åä¸€æ¡é€šå¸¸æ²¡æœ‰ä¸‹ä¸€æ—¶åˆ»ç›®æ ‡ï¼‰
    feat_df = feat_df.dropna(subset=['target_next'])
    
    print(f"ğŸ“Š æœ€ç»ˆç‰¹å¾ç»´åº¦: {feat_df.shape[1]-1} ä¸ªç‰¹å¾ï¼ˆä¸åŒ…æ‹¬ç›®æ ‡å˜é‡ï¼‰")
    print(f"ğŸ“Š ç‰¹å¾åç§°: {[col for col in feat_df.columns if col != 'target_next']}")
    
    return feat_df


def build_sequences_from_features(feat_df, seq_days=1, step_per_day=96):
    """
    æ„å»ºLSTMè¾“å…¥åºåˆ—ï¼šä»¥æ—¶é—´åºåˆ—é¡ºåºç”¨è¿‡å» seq_days*step_per_day ä¸ªæ—¶åˆ»çš„ç‰¹å¾é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶åˆ»ç›®æ ‡ã€‚
    é»˜è®¤ä¸åšå·¦ç«¯ paddingï¼Œåªæœ‰å†å²é•¿åº¦ >= seq_len æ—¶æ‰æ„å»ºæ ·æœ¬ï¼Œé¿å…ä½¿ç”¨é‡å¤å¡«å……å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°å¸¸æ•°æ¨¡å¼ã€‚
    è‹¥ç¡®å®éœ€è¦ä¿ç•™æ—©æœŸæ ·æœ¬ï¼Œå¯å°†ä¸‹é¢çš„é€»è¾‘æ”¹å› padding ç­–ç•¥æˆ–å®ç°æ›´åˆç†çš„å¡«å……ï¼ˆå¦‚åŸºäºçª—å£å‡å€¼æˆ–é•œåƒå¡«å……ï¼‰ã€‚
    """
    seq_len = seq_days * step_per_day
    X, y, dates = [], [], []
    values = feat_df.values
    cols = feat_df.columns.tolist()
    target_idx = cols.index('target_next')

    n = len(values)
    for i in range(n):
        # ä»…åœ¨æœ‰è¶³å¤Ÿå†å²ï¼ˆå®Œæ•´çª—å£ï¼‰æ—¶æ„å»ºæ ·æœ¬ï¼Œé¿å…å·¦ä¾§é‡å¤å¡«å……
        start = i - seq_len
        if start < 0:
            # è·³è¿‡æ—©æœŸæ ·æœ¬
            continue
        seq = values[start:i, :target_idx]

        X.append(seq)
        y.append(values[i, target_idx])
        dates.append(feat_df.index[i])

    X = np.array(X)
    y = np.array(y)
    return X, y, dates


def time_order_split(X, y, dates, train_frac=0.7, test_frac=0.15):
    n = len(X)
    n_train = int(n * train_frac)
    n_test = int(n * test_frac)
    n_val = n - n_train - n_test
    X_train = X[:n_train]; y_train = y[:n_train]
    X_test = X[n_train:n_train + n_test]; y_test = y[n_train:n_train + n_test]
    X_val = X[n_train + n_test:]; y_val = y[n_train + n_test:]
    dates_train = dates[:n_train]; dates_test = dates[n_train:n_train + n_test]; dates_val = dates[n_train + n_test:]
    return (X_train, y_train, dates_train), (X_test, y_test, dates_test), (X_val, y_val, dates_val)


def build_lstm_model(input_shape):
    inp = layers.Input(shape=input_shape)
    x = layers.LSTM(64, return_sequences=False)(inp)
    x = layers.Dense(32, activation='relu')(x)
    out = layers.Dense(1)(x)
    model = models.Model(inp, out)
    model.compile(optimizer='adam', loss='mse')
    return model


def crps_gaussian(y_true, mu, sigma):
    # CRPS for Gaussian forecast with mean mu and std sigma
    # vectorized implementation
    z = (y_true - mu) / (sigma + 1e-12)
    pdf = norm.pdf(z)
    cdf = norm.cdf(z)
    crps = sigma * (z * (2 * cdf - 1) + 2 * pdf - 1.0 / math.sqrt(math.pi))
    return np.mean(crps)


def coverage_probability(y_true, mu, sigma, alpha=0.95):
    z = norm.ppf((1 + alpha) / 2.0)
    lower = mu - z * sigma
    upper = mu + z * sigma
    return np.mean((y_true >= lower) & (y_true <= upper))


def mbe(y_true, y_pred):
    return np.mean(y_pred - y_true)


def hmm_load_segmentation(load_values, n_states='auto', min_states=3, max_states=5, min_segment_length=8):
    """
    ä½¿ç”¨éšé©¬å°”å¯å¤«æ¨¡å‹å¯¹è´Ÿè·æ›²çº¿è¿›è¡Œæ™ºèƒ½åˆ†æ®µï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è¿‡åº¦åˆ†æ®µï¼‰
    
    å‚æ•°:
    - load_values: è´Ÿè·å€¼åºåˆ—ï¼ˆä¸€ç»´æ•°ç»„ï¼‰
    - n_states: çŠ¶æ€æ•°é‡ï¼Œ'auto'ä¸ºè‡ªåŠ¨é€‰æ‹©ï¼Œæˆ–æŒ‡å®šæ•´æ•°
    - min_states: è‡ªåŠ¨é€‰æ‹©æ—¶çš„æœ€å°çŠ¶æ€æ•°
    - max_states: è‡ªåŠ¨é€‰æ‹©æ—¶çš„æœ€å¤§çŠ¶æ€æ•°
    - min_segment_length: æœ€å°æ®µé•¿åº¦ï¼ˆæ—¶é—´ç‚¹æ•°ï¼‰
    
    è¿”å›:
    - states: æ¯ä¸ªæ—¶é—´ç‚¹å¯¹åº”çš„çŠ¶æ€åºåˆ—
    - state_means: æ¯ä¸ªçŠ¶æ€çš„å¹³å‡è´Ÿè·æ°´å¹³
    - segments: è¿ç»­æ®µä¿¡æ¯ [(start_idx, end_idx, state, mean_load), ...]
    """
    # å¦‚æœhmmlearnä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨ç®€å•åˆ†æ®µæ–¹æ³•
    if not HMM_AVAILABLE:
        return simple_load_segmentation(load_values, n_segments=4)
    
    try:
        # æ•°æ®é¢„å¤„ç†
        load_values = np.array(load_values).reshape(-1, 1)
        
        # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çŠ¶æ€æ•°ï¼ˆå‡å°‘æœ€å¤§çŠ¶æ€æ•°ï¼‰
        if n_states == 'auto':
            best_score = -np.inf
            best_n_states = min_states
            
            for n in range(min_states, max_states + 1):
                try:
                    # ä½¿ç”¨GaussianHMMè¿›è¡Œè®­ç»ƒ
                    model = hmm.GaussianHMM(n_components=n, covariance_type="full", random_state=42)
                    model.fit(load_values)
                    score = model.score(load_values)
                    
                    if score > best_score:
                        best_score = score
                        best_n_states = n
                except:
                    continue
            
            n_states = best_n_states
        
        # è®­ç»ƒæœ€ç»ˆçš„HMMæ¨¡å‹
        model = hmm.GaussianHMM(n_components=n_states, covariance_type="full", random_state=42)
        
        # è®¾ç½®æ›´é«˜çš„è‡ªè½¬ç§»æ¦‚ç‡ï¼Œå‡å°‘çŠ¶æ€åˆ‡æ¢
        transition_prob = 0.98  # æé«˜ä¿æŒå½“å‰çŠ¶æ€çš„æ¦‚ç‡
        transfer_prob = (1 - transition_prob) / (n_states - 1)  # è½¬ç§»åˆ°å…¶ä»–çŠ¶æ€çš„æ¦‚ç‡
        
        transmat = np.full((n_states, n_states), transfer_prob)
        np.fill_diagonal(transmat, transition_prob)
        model.transmat_ = transmat
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(load_values)
        
        # é¢„æµ‹çŠ¶æ€åºåˆ—
        raw_states = model.predict(load_values)
        
        # åº”ç”¨ä¸­å€¼æ»¤æ³¢å‡å°‘å™ªå£°
        from scipy import ndimage
        smoothed_states = ndimage.median_filter(raw_states.astype(float), size=5).astype(int)
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„å¹³å‡è´Ÿè·æ°´å¹³
        state_means = []
        for state in range(n_states):
            state_mask = (smoothed_states == state)
            if np.any(state_mask):
                state_mean = np.mean(load_values[state_mask])
                state_means.append(state_mean)
            else:
                state_means.append(0)
        
        state_means = np.array(state_means).flatten()
        
        # æ ¹æ®å¹³å‡è´Ÿè·æ°´å¹³å¯¹çŠ¶æ€è¿›è¡Œæ’åºï¼ˆä»ä½åˆ°é«˜ï¼‰
        sorted_indices = np.argsort(state_means)
        state_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted_indices)}
        
        # é‡æ–°æ˜ å°„çŠ¶æ€
        mapped_states = np.array([state_mapping[s] for s in smoothed_states])
        sorted_state_means = state_means[sorted_indices]
        
        # è¯†åˆ«åˆå§‹è¿ç»­æ®µ
        initial_segments = []
        current_state = mapped_states[0]
        start_idx = 0
        
        for i in range(1, len(mapped_states)):
            if mapped_states[i] != current_state:
                # å½“å‰æ®µç»“æŸ
                end_idx = i - 1
                segment_load = np.mean(load_values[start_idx:i])
                initial_segments.append((start_idx, end_idx, current_state, segment_load))
                
                # å¼€å§‹æ–°æ®µ
                start_idx = i
                current_state = mapped_states[i]
        
        # æ·»åŠ æœ€åä¸€æ®µ
        segment_load = np.mean(load_values[start_idx:])
        initial_segments.append((start_idx, len(mapped_states) - 1, current_state, segment_load))
        
        # åå¤„ç†ï¼šåˆå¹¶çŸ­å°æ®µè½
        merged_segments = merge_short_segments(initial_segments, load_values, min_segment_length)
        
        # é‡æ–°æ„å»ºçŠ¶æ€åºåˆ—
        final_states = np.zeros_like(mapped_states)
        for start_idx, end_idx, state, _ in merged_segments:
            final_states[start_idx:end_idx+1] = state
        
        # é‡æ–°è®¡ç®—çŠ¶æ€å¹³å‡å€¼
        final_state_means = []
        unique_states = sorted(set([seg[2] for seg in merged_segments]))
        for state in unique_states:
            state_mask = (final_states == state)
            if np.any(state_mask):
                state_mean = np.mean(load_values[state_mask])
                final_state_means.append(state_mean)
            else:
                final_state_means.append(0)
        
        final_state_means = np.array(final_state_means).flatten()
        
        return final_states, final_state_means, merged_segments
        
    except Exception as e:
        print(f"âŒ HMMåˆ†æ®µå¤±è´¥: {e}")
        # é€€å›åˆ°ç®€å•çš„åˆ†æ®µæ–¹æ³•
        return simple_load_segmentation(load_values.flatten(), n_segments=4)

def merge_short_segments(segments, load_values, min_segment_length=8):
    """
    åˆå¹¶è¿‡çŸ­çš„æ®µè½ï¼Œå‡å°‘è¿‡åº¦åˆ†å‰²
    
    å‚æ•°:
    - segments: åˆå§‹æ®µè½åˆ—è¡¨ [(start, end, state, mean_load), ...]
    - load_values: è´Ÿè·å€¼æ•°ç»„
    - min_segment_length: æœ€å°æ®µè½é•¿åº¦
    
    è¿”å›:
    - merged_segments: åˆå¹¶åçš„æ®µè½åˆ—è¡¨
    """
    if len(segments) <= 1:
        return segments
    
    merged = []
    i = 0
    
    while i < len(segments):
        start_idx, end_idx, state, mean_load = segments[i]
        segment_length = end_idx - start_idx + 1
        
        # å¦‚æœå½“å‰æ®µå¤ªçŸ­ï¼Œå°è¯•ä¸ç›¸é‚»æ®µåˆå¹¶
        if segment_length < min_segment_length and len(merged) > 0:
            # ä¸å‰ä¸€ä¸ªæ®µåˆå¹¶
            prev_start, prev_end, prev_state, prev_mean = merged[-1]
            
            # è®¡ç®—åˆå¹¶åçš„å¹³å‡è´Ÿè·
            combined_load = np.mean(load_values[prev_start:end_idx+1])
            
            # å†³å®šä½¿ç”¨å“ªä¸ªçŠ¶æ€ï¼ˆé€‰æ‹©è´Ÿè·æ°´å¹³æ›´æ¥è¿‘åˆå¹¶åå¹³å‡å€¼çš„çŠ¶æ€ï¼‰
            if abs(prev_mean - combined_load) <= abs(mean_load - combined_load):
                final_state = prev_state
            else:
                final_state = state
            
            # æ›´æ–°æœ€åä¸€ä¸ªæ®µ
            merged[-1] = (prev_start, end_idx, final_state, combined_load)
            
        elif segment_length < min_segment_length and i < len(segments) - 1:
            # ä¸ä¸‹ä¸€ä¸ªæ®µåˆå¹¶
            next_start, next_end, next_state, next_mean = segments[i + 1]
            
            # è®¡ç®—åˆå¹¶åçš„å¹³å‡è´Ÿè·
            combined_load = np.mean(load_values[start_idx:next_end+1])
            
            # å†³å®šä½¿ç”¨å“ªä¸ªçŠ¶æ€
            if abs(mean_load - combined_load) <= abs(next_mean - combined_load):
                final_state = state
            else:
                final_state = next_state
            
            # æ·»åŠ åˆå¹¶åçš„æ®µ
            merged.append((start_idx, next_end, final_state, combined_load))
            i += 1  # è·³è¿‡ä¸‹ä¸€ä¸ªæ®µï¼Œå› ä¸ºå·²ç»åˆå¹¶äº†
            
        else:
            # æ®µé•¿åº¦è¶³å¤Ÿï¼Œç›´æ¥æ·»åŠ 
            merged.append((start_idx, end_idx, state, mean_load))
        
        i += 1
    
    # å¦‚æœè¿˜æœ‰å¾ˆçŸ­çš„æ®µï¼Œè¿›è¡ŒäºŒæ¬¡åˆå¹¶
    if len(merged) > 1:
        final_merged = []
        for seg in merged:
            start_idx, end_idx, state, mean_load = seg
            segment_length = end_idx - start_idx + 1
            
            if segment_length < min_segment_length // 2 and len(final_merged) > 0:
                # ä¸å‰ä¸€ä¸ªæ®µåˆå¹¶
                prev_start, prev_end, prev_state, prev_mean = final_merged[-1]
                combined_load = np.mean(load_values[prev_start:end_idx+1])
                final_merged[-1] = (prev_start, end_idx, prev_state, combined_load)
            else:
                final_merged.append(seg)
        
        return final_merged
    
    return merged

def explain_load_changes(segments, feat_df, pred_times, load_values):
    """
    è´Ÿè·å˜åŒ–å¯è§£é‡Šæ€§æ¨¡å‹ - åˆ†æè´Ÿè·é˜¶æ®µå˜åŒ–çš„åŸå› 
    
    å‚æ•°:
    - segments: è´Ÿè·åˆ†æ®µä¿¡æ¯ [(start_idx, end_idx, state, mean_load), ...]
    - feat_df: ç‰¹å¾æ•°æ®æ¡†
    - pred_times: é¢„æµ‹æ—¶é—´ç‚¹åˆ—è¡¨
    - load_values: è´Ÿè·å€¼æ•°ç»„
    
    è¿”å›:
    - explanations: åŒ…å«å„é˜¶æ®µå˜åŒ–è§£é‡Šçš„å­—å…¸
    """
    try:
        explanations = {
            'segment_analysis': [],
            'trend_analysis': {},
            'feature_importance': {},
            'environmental_impact': {}
        }
        
        # æ£€æŸ¥å¯ç”¨çš„ç¯å¢ƒç‰¹å¾
        env_features = ['temperature', 'humidity', 'visibility', 'pressure', 'windSpeed', 'cloudCover', 'dewPoint']
        available_env_features = []
        for feat in env_features:
            if f'{feat}_current' in feat_df.columns:
                available_env_features.append(feat)
        
        # 1. é€æ®µåˆ†æè´Ÿè·ç‰¹å¾å’Œå˜åŒ–åŸå› 
        for i, (start_idx, end_idx, state, mean_load) in enumerate(segments):
            segment_info = {
                'segment_id': i + 1,
                'start_time': start_idx * 15 / 60,  # è½¬æ¢ä¸ºå°æ—¶
                'end_time': (end_idx + 1) * 15 / 60,
                'duration_hours': (end_idx - start_idx + 1) * 15 / 60,
                'state': int(state),
                'mean_load': float(mean_load),
                'load_level': '',
                'key_factors': []
            }
            
            # ç¡®å®šè´Ÿè·æ°´å¹³ç±»åˆ«
            all_segment_means = [seg[3] for seg in segments]
            load_percentile = (sorted(all_segment_means).index(mean_load) + 1) / len(all_segment_means)
            
            if load_percentile <= 0.25:
                segment_info['load_level'] = 'ä½è´Ÿè·'
            elif load_percentile <= 0.5:
                segment_info['load_level'] = 'ä¸­ä½è´Ÿè·'
            elif load_percentile <= 0.75:
                segment_info['load_level'] = 'ä¸­é«˜è´Ÿè·'
            else:
                segment_info['load_level'] = 'é«˜è´Ÿè·'
            
            # æå–è¯¥æ®µçš„ç‰¹å¾æ•°æ®ï¼ˆå¦‚æœæ—¶é—´å¯¹é½ï¼‰
            if pred_times and len(pred_times) > end_idx:
                try:
                    # è·å–è¯¥æ®µæ—¶é—´èŒƒå›´å†…çš„ç‰¹å¾æ•°æ®
                    segment_times = pred_times[start_idx:end_idx+1]
                    
                    # æŸ¥æ‰¾ç‰¹å¾æ•°æ®ä¸­å¯¹åº”çš„æ—¶é—´ç‚¹
                    matching_features = []
                    for t in segment_times:
                        # æ‰¾åˆ°æœ€æ¥è¿‘çš„ç‰¹å¾æ•°æ®æ—¶é—´ç‚¹
                        idx = feat_df.index.get_indexer([t], method='nearest')[0]
                        if 0 <= idx < len(feat_df):
                            matching_features.append(feat_df.iloc[idx])
                    
                    if matching_features:
                        # è®¡ç®—è¯¥æ®µçš„å¹³å‡ç‰¹å¾å€¼
                        segment_features = pd.DataFrame(matching_features)
                        
                        # åˆ†æç¯å¢ƒå› ç´ çš„å½±å“
                        for env_feat in available_env_features:
                            current_col = f'{env_feat}_current'
                            if current_col in segment_features.columns:
                                avg_value = segment_features[current_col].mean()
                                
                                # æ ¹æ®ç‰¹å¾å€¼åˆ¤æ–­å½±å“
                                if env_feat == 'temperature':
                                    if avg_value > 25:
                                        segment_info['key_factors'].append(f'é«˜æ¸©({avg_value:.1f}Â°C)å¯èƒ½å¢åŠ ç©ºè°ƒè´Ÿè·')
                                    elif avg_value < 10:
                                        segment_info['key_factors'].append(f'ä½æ¸©({avg_value:.1f}Â°C)å¯èƒ½å¢åŠ ä¾›æš–è´Ÿè·')
                                    else:
                                        segment_info['key_factors'].append(f'æ¸©åº¦é€‚ä¸­({avg_value:.1f}Â°C)')
                                
                                elif env_feat == 'humidity':
                                    if avg_value > 70:
                                        segment_info['key_factors'].append(f'é«˜æ¹¿åº¦({avg_value:.1f}%)å¯èƒ½å¢åŠ é™¤æ¹¿éœ€æ±‚')
                                    elif avg_value < 30:
                                        segment_info['key_factors'].append(f'ä½æ¹¿åº¦({avg_value:.1f}%)')
                                
                                elif env_feat == 'cloudCover':
                                    if avg_value > 0.7:
                                        segment_info['key_factors'].append(f'å¤šäº‘({avg_value:.2f})å‡å°‘è‡ªç„¶é‡‡å…‰')
                                    elif avg_value < 0.3:
                                        segment_info['key_factors'].append(f'æ™´æœ—({avg_value:.2f})å¢åŠ è‡ªç„¶é‡‡å…‰')
                        
                        # åˆ†ææ—¶é—´ç‰¹å¾çš„å½±å“
                        if 'hour' in segment_features.columns:
                            avg_hour = segment_features['hour'].mean()
                            if 6 <= avg_hour < 9:
                                segment_info['key_factors'].append('æ—©é«˜å³°æ—¶æ®µ - èµ·åºŠã€æ—©é¤æ´»åŠ¨')
                            elif 9 <= avg_hour < 12:
                                segment_info['key_factors'].append('ä¸Šåˆæ—¶æ®µ - å¤šæ•°å®¶åº­æˆå‘˜å¤–å‡º')
                            elif 12 <= avg_hour < 14:
                                segment_info['key_factors'].append('åˆé—´æ—¶æ®µ - åˆé¤ã€ä¼‘æ¯')
                            elif 14 <= avg_hour < 18:
                                segment_info['key_factors'].append('ä¸‹åˆæ—¶æ®µ - æŒç»­ä½è´Ÿè·')
                            elif 18 <= avg_hour < 22:
                                segment_info['key_factors'].append('æ™šé«˜å³°æ—¶æ®µ - å›å®¶ã€æ™šé¤ã€å¨±ä¹')
                            elif 22 <= avg_hour or avg_hour < 6:
                                segment_info['key_factors'].append('å¤œé—´æ—¶æ®µ - ç¡çœ ã€å¾…æœºè´Ÿè·')
                        
                        # åˆ†æè´Ÿè·å˜åŒ–ç‡
                        if 'load_smooth' in segment_features.columns:
                            load_std = segment_features['load_smooth'].std()
                            if load_std > 0.2:
                                segment_info['key_factors'].append(f'è´Ÿè·æ³¢åŠ¨è¾ƒå¤§(æ ‡å‡†å·®={load_std:.3f})')
                            else:
                                segment_info['key_factors'].append(f'è´Ÿè·ç›¸å¯¹ç¨³å®š(æ ‡å‡†å·®={load_std:.3f})')
                
                except Exception as e:
                    print(f"âš ï¸ åˆ†æé˜¶æ®µ {i+1} ç‰¹å¾æ—¶å‡ºé”™: {e}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“å› ç´ ï¼Œæ·»åŠ é€šç”¨è¯´æ˜
            if not segment_info['key_factors']:
                segment_info['key_factors'].append('è´Ÿè·æ°´å¹³ä¸»è¦ç”±ç”¨æˆ·è¡Œä¸ºæ¨¡å¼å†³å®š')
            
            explanations['segment_analysis'].append(segment_info)
        
        # 2. åˆ†æé˜¶æ®µé—´çš„è¶‹åŠ¿å˜åŒ–
        if len(segments) > 1:
            trend_changes = []
            for i in range(len(segments) - 1):
                curr_seg = segments[i]
                next_seg = segments[i + 1]
                
                load_change = next_seg[3] - curr_seg[3]
                load_change_pct = (load_change / curr_seg[3] * 100) if curr_seg[3] != 0 else 0
                
                trend_info = {
                    'from_segment': i + 1,
                    'to_segment': i + 2,
                    'load_change': float(load_change),
                    'load_change_percent': float(load_change_pct),
                    'trend': '',
                    'explanation': []
                }
                
                # åˆ¤æ–­å˜åŒ–è¶‹åŠ¿
                if abs(load_change_pct) < 5:
                    trend_info['trend'] = 'ç¨³å®š'
                    trend_info['explanation'].append('è´Ÿè·æ°´å¹³åŸºæœ¬ä¿æŒä¸å˜')
                elif load_change_pct > 0:
                    if load_change_pct > 30:
                        trend_info['trend'] = 'æ˜¾è‘—ä¸Šå‡'
                        trend_info['explanation'].append(f'è´Ÿè·å¤§å¹…å¢åŠ {load_change_pct:.1f}%')
                    else:
                        trend_info['trend'] = 'ä¸Šå‡'
                        trend_info['explanation'].append(f'è´Ÿè·å¢åŠ {load_change_pct:.1f}%')
                else:
                    if load_change_pct < -30:
                        trend_info['trend'] = 'æ˜¾è‘—ä¸‹é™'
                        trend_info['explanation'].append(f'è´Ÿè·å¤§å¹…ä¸‹é™{abs(load_change_pct):.1f}%')
                    else:
                        trend_info['trend'] = 'ä¸‹é™'
                        trend_info['explanation'].append(f'è´Ÿè·ä¸‹é™{abs(load_change_pct):.1f}%')
                
                # å°è¯•è§£é‡Šå˜åŒ–åŸå› ï¼ˆåŸºäºæ—¶é—´å’Œç¯å¢ƒï¼‰
                curr_start_hour = curr_seg[0] * 15 / 60
                next_start_hour = next_seg[0] * 15 / 60
                
                # æ—¶é—´ç›¸å…³çš„å˜åŒ–è§£é‡Š
                if curr_start_hour < 6 and next_start_hour >= 6:
                    trend_info['explanation'].append('è¿›å…¥æ—©æ™¨æ—¶æ®µï¼Œå®¶åº­æ´»åŠ¨å¢åŠ ')
                elif curr_start_hour < 18 and next_start_hour >= 18:
                    trend_info['explanation'].append('è¿›å…¥å‚æ™šæ—¶æ®µï¼Œå®¶åº­æˆå‘˜è¿”å›')
                elif curr_start_hour < 22 and next_start_hour >= 22:
                    trend_info['explanation'].append('è¿›å…¥æ·±å¤œæ—¶æ®µï¼Œæ´»åŠ¨å‡å°‘')
                elif curr_start_hour >= 9 and next_start_hour < 18:
                    trend_info['explanation'].append('æ—¥é—´æ—¶æ®µï¼Œå¤šæ•°å®¶åº­æˆå‘˜å¤–å‡ºå·¥ä½œ')
                
                trend_changes.append(trend_info)
            
            explanations['trend_analysis'] = {
                'total_segments': len(segments),
                'transitions': trend_changes,
                'max_load': float(max([seg[3] for seg in segments])),
                'min_load': float(min([seg[3] for seg in segments])),
                'load_range': float(max([seg[3] for seg in segments]) - min([seg[3] for seg in segments]))
            }
        
        # 3. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆåŸºäºç‰¹å¾å˜åŒ–ä¸è´Ÿè·å˜åŒ–çš„ç›¸å…³æ€§ï¼‰
        if available_env_features:
            feature_correlations = {}
            
            for env_feat in available_env_features:
                current_col = f'{env_feat}_current'
                if current_col in feat_df.columns:
                    try:
                        # ç®€å•çš„ç›¸å…³æ€§åˆ†æ
                        if 'load_smooth' in feat_df.columns:
                            valid_indices = ~(feat_df[current_col].isna() | feat_df['load_smooth'].isna())
                            if valid_indices.sum() > 10:
                                correlation = feat_df.loc[valid_indices, current_col].corr(
                                    feat_df.loc[valid_indices, 'load_smooth']
                                )
                                feature_correlations[env_feat] = float(correlation)
                    except Exception as e:
                        print(f"âš ï¸ è®¡ç®— {env_feat} ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
            
            # æ’åºç‰¹å¾é‡è¦æ€§
            sorted_features = sorted(feature_correlations.items(), key=lambda x: abs(x[1]), reverse=True)
            
            explanations['feature_importance'] = {
                'correlations': dict(sorted_features),
                'top_features': [f[0] for f in sorted_features[:3]],
                'interpretation': []
            }
            
            for feat, corr in sorted_features[:3]:
                if abs(corr) > 0.3:
                    direction = 'æ­£ç›¸å…³' if corr > 0 else 'è´Ÿç›¸å…³'
                    explanations['feature_importance']['interpretation'].append(
                        f'{feat}ä¸è´Ÿè·å‘ˆ{direction}(ç›¸å…³ç³»æ•°={corr:.3f})'
                    )
        
        # 4. ç¯å¢ƒå› ç´ ç»¼åˆå½±å“è¯„ä¼°
        if available_env_features and pred_times:
            env_impact = {}
            
            for env_feat in available_env_features:
                current_col = f'{env_feat}_current'
                if current_col in feat_df.columns:
                    try:
                        # è®¡ç®—å…¨å¤©è¯¥ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
                        values = []
                        for t in pred_times:
                            idx = feat_df.index.get_indexer([t], method='nearest')[0]
                            if 0 <= idx < len(feat_df):
                                val = feat_df.iloc[idx][current_col]
                                if not np.isnan(val):
                                    values.append(val)
                        
                        if values:
                            env_impact[env_feat] = {
                                'mean': float(np.mean(values)),
                                'std': float(np.std(values)),
                                'min': float(np.min(values)),
                                'max': float(np.max(values)),
                                'range': float(np.max(values) - np.min(values))
                            }
                    except Exception as e:
                        print(f"âš ï¸ åˆ†æ {env_feat} å½±å“æ—¶å‡ºé”™: {e}")
            
            explanations['environmental_impact'] = env_impact
        
        return explanations
        
    except Exception as e:
        print(f"âŒ è´Ÿè·å˜åŒ–è§£é‡Šåˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'segment_analysis': [],
            'trend_analysis': {},
            'feature_importance': {},
            'environmental_impact': {},
            'error': str(e)
        }

def compare_with_historical_stages(current_segments, historical_segments, 
                                   current_feat_df, historical_feat_df,
                                   current_times, historical_times,
                                   current_load, historical_load):
    """
    ä¸å†å²è´Ÿè·é˜¶æ®µè¿›è¡Œå¯¹æ¯”åˆ†æ
    
    å‚æ•°:
    - current_segments: å½“å‰è´Ÿè·åˆ†æ®µä¿¡æ¯ [(start_idx, end_idx, state, mean_load), ...]
    - historical_segments: å†å²è´Ÿè·åˆ†æ®µä¿¡æ¯ [(start_idx, end_idx, state, mean_load), ...]
    - current_feat_df: å½“å‰ç‰¹å¾æ•°æ®æ¡†
    - historical_feat_df: å†å²ç‰¹å¾æ•°æ®æ¡†
    - current_times: å½“å‰æ—¶é—´ç‚¹åˆ—è¡¨
    - historical_times: å†å²æ—¶é—´ç‚¹åˆ—è¡¨
    - current_load: å½“å‰è´Ÿè·å€¼æ•°ç»„
    - historical_load: å†å²è´Ÿè·å€¼æ•°ç»„
    
    è¿”å›:
    - comparison: åŒ…å«å¯¹æ¯”åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        comparison = {
            'stage_count_comparison': {},
            'aligned_stages': [],
            'significant_differences': [],
            'behavior_explanations': []
        }
        
        # 1. åˆ†æé˜¶æ®µæ•°é‡å˜åŒ–
        current_count = len(current_segments)
        historical_count = len(historical_segments)
        count_change = current_count - historical_count
        count_change_pct = (count_change / historical_count * 100) if historical_count > 0 else 0
        
        comparison['stage_count_comparison'] = {
            'current_count': current_count,
            'historical_count': historical_count,
            'change': count_change,
            'change_percent': count_change_pct,
            'trend': 'å¢åŠ ' if count_change > 0 else ('å‡å°‘' if count_change < 0 else 'ä¸å˜'),
            'reasons': []
        }
        
        # è§£é‡Šé˜¶æ®µæ•°é‡å˜åŒ–çš„åŸå› 
        if count_change > 0:
            comparison['stage_count_comparison']['reasons'].append(
                f'è´Ÿè·é˜¶æ®µæ•°å¢åŠ {abs(count_change)}ä¸ªï¼Œå¯èƒ½åŸå› ï¼š'
            )
            comparison['stage_count_comparison']['reasons'].append(
                '  1. ç”¨ç”µè¡Œä¸ºæ›´åŠ å¤šæ ·åŒ–ï¼Œå‡ºç°æ›´å¤šè´Ÿè·åˆ‡æ¢'
            )
            comparison['stage_count_comparison']['reasons'].append(
                '  2. å®¶åº­æˆå‘˜æ´»åŠ¨æ¨¡å¼å‘ç”Ÿå˜åŒ–'
            )
            comparison['stage_count_comparison']['reasons'].append(
                '  3. æ–°å¢ç”¨ç”µè®¾å¤‡æˆ–æ”¹å˜ä½¿ç”¨ä¹ æƒ¯'
            )
        elif count_change < 0:
            comparison['stage_count_comparison']['reasons'].append(
                f'è´Ÿè·é˜¶æ®µæ•°å‡å°‘{abs(count_change)}ä¸ªï¼Œå¯èƒ½åŸå› ï¼š'
            )
            comparison['stage_count_comparison']['reasons'].append(
                '  1. ç”¨ç”µè¡Œä¸ºæ›´åŠ è§„å¾‹ï¼Œè´Ÿè·æ¨¡å¼ç®€åŒ–'
            )
            comparison['stage_count_comparison']['reasons'].append(
                '  2. å®¶åº­æˆå‘˜å‡å°‘æˆ–å¤–å‡ºæ—¶é—´å¢åŠ '
            )
            comparison['stage_count_comparison']['reasons'].append(
                '  3. å‡å°‘äº†ç”¨ç”µè®¾å¤‡ä½¿ç”¨æˆ–ä¼˜åŒ–äº†ç”¨ç”µä¹ æƒ¯'
            )
        else:
            comparison['stage_count_comparison']['reasons'].append(
                'è´Ÿè·é˜¶æ®µæ•°ä¿æŒä¸å˜ï¼Œç”¨ç”µæ¨¡å¼ç›¸å¯¹ç¨³å®š'
            )
        
        # 2. é€é˜¶æ®µå¯¹é½åˆ†æï¼ˆåŸºäºæ—¶é—´å’Œè´Ÿè·æ°´å¹³ï¼‰
        # ä½¿ç”¨ç®€å•çš„æ—¶é—´é‡å åŒ¹é…ç­–ç•¥
        aligned_pairs = []
        
        for curr_idx, (curr_start, curr_end, curr_state, curr_mean) in enumerate(current_segments):
            curr_start_hour = curr_start * 15 / 60
            curr_end_hour = (curr_end + 1) * 15 / 60
            curr_mid_hour = (curr_start_hour + curr_end_hour) / 2
            
            # æ‰¾åˆ°å†å²æ•°æ®ä¸­æ—¶é—´æœ€æ¥è¿‘çš„é˜¶æ®µ
            best_match = None
            best_overlap = 0
            best_time_diff = float('inf')
            
            for hist_idx, (hist_start, hist_end, hist_state, hist_mean) in enumerate(historical_segments):
                hist_start_hour = hist_start * 15 / 60
                hist_end_hour = (hist_end + 1) * 15 / 60
                hist_mid_hour = (hist_start_hour + hist_end_hour) / 2
                
                # è®¡ç®—æ—¶é—´é‡å 
                overlap_start = max(curr_start_hour, hist_start_hour)
                overlap_end = min(curr_end_hour, hist_end_hour)
                overlap = max(0, overlap_end - overlap_start)
                
                # è®¡ç®—ä¸­å¿ƒç‚¹æ—¶é—´å·®
                time_diff = abs(curr_mid_hour - hist_mid_hour)
                
                # é€‰æ‹©é‡å æœ€å¤§æˆ–æ—¶é—´æœ€æ¥è¿‘çš„é˜¶æ®µ
                if overlap > best_overlap or (overlap == best_overlap and time_diff < best_time_diff):
                    best_overlap = overlap
                    best_time_diff = time_diff
                    best_match = hist_idx
            
            if best_match is not None:
                hist_start, hist_end, hist_state, hist_mean = historical_segments[best_match]
                aligned_pairs.append((curr_idx, best_match))
                
                # è®¡ç®—è´Ÿè·å·®å¼‚
                load_diff = curr_mean - hist_mean
                load_diff_pct = (load_diff / hist_mean * 100) if hist_mean != 0 else 0
                
                # æå–ç¯å¢ƒç‰¹å¾å·®å¼‚
                env_diff = {}
                try:
                    # è·å–å½“å‰é˜¶æ®µçš„ç¯å¢ƒç‰¹å¾
                    if current_times and len(current_times) > curr_end:
                        curr_segment_times = current_times[curr_start:curr_end+1]
                        curr_features = []
                        for t in curr_segment_times:
                            idx = current_feat_df.index.get_indexer([t], method='nearest')[0]
                            if 0 <= idx < len(current_feat_df):
                                curr_features.append(current_feat_df.iloc[idx])
                        
                        if curr_features:
                            curr_feat_mean = pd.DataFrame(curr_features).mean()
                            
                            # è·å–å†å²é˜¶æ®µçš„ç¯å¢ƒç‰¹å¾
                            if historical_times and len(historical_times) > hist_end:
                                hist_segment_times = historical_times[hist_start:hist_end+1]
                                hist_features = []
                                for t in hist_segment_times:
                                    idx = historical_feat_df.index.get_indexer([t], method='nearest')[0]
                                    if 0 <= idx < len(historical_feat_df):
                                        hist_features.append(historical_feat_df.iloc[idx])
                                
                                if hist_features:
                                    hist_feat_mean = pd.DataFrame(hist_features).mean()
                                    
                                    # è®¡ç®—ä¸»è¦ç¯å¢ƒç‰¹å¾çš„å·®å¼‚
                                    for feat in ['temperature_current', 'humidity_current', 'cloudCover_current']:
                                        if feat in curr_feat_mean.index and feat in hist_feat_mean.index:
                                            env_diff[feat] = {
                                                'current': float(curr_feat_mean[feat]),
                                                'historical': float(hist_feat_mean[feat]),
                                                'diff': float(curr_feat_mean[feat] - hist_feat_mean[feat])
                                            }
                except Exception as e:
                    print(f"âš ï¸ æå–ç¯å¢ƒç‰¹å¾å·®å¼‚æ—¶å‡ºé”™: {e}")
                
                # è®¡ç®—æ—¶é—´åç§»ï¼ˆé˜¶æ®µçš„å·¦ç§»æˆ–å³ç§»ï¼‰
                hist_start_hour = hist_start * 15 / 60
                hist_end_hour = (hist_end + 1) * 15 / 60
                hist_mid_hour = (hist_start_hour + hist_end_hour) / 2
                
                # ä½¿ç”¨é˜¶æ®µçš„ä¸­å¿ƒç‚¹æ—¶é—´æ¥åˆ¤æ–­æ•´ä½“åç§»
                time_shift = curr_mid_hour - hist_mid_hour
                
                aligned_stage = {
                    'current_stage': curr_idx + 1,
                    'historical_stage': best_match + 1,
                    'current_time_range': f"{curr_start_hour:.1f}h-{curr_end_hour:.1f}h",
                    'historical_time_range': f"{hist_start_hour:.1f}h-{hist_end_hour:.1f}h",
                    'current_load': float(curr_mean),
                    'historical_load': float(hist_mean),
                    'load_difference': float(load_diff),
                    'load_difference_percent': float(load_diff_pct),
                    'time_overlap': float(best_overlap),
                    'time_shift': float(time_shift),  # æ­£å€¼è¡¨ç¤ºå³ç§»ï¼ˆæ¨è¿Ÿï¼‰ï¼Œè´Ÿå€¼è¡¨ç¤ºå·¦ç§»ï¼ˆæå‰ï¼‰
                    'environment_diff': env_diff
                }
                
                comparison['aligned_stages'].append(aligned_stage)
        
        # 3. è¯†åˆ«å·®å¼‚è¾ƒå¤§çš„è´Ÿè·é˜¶æ®µï¼ˆè´Ÿè·å·®å¼‚ > 20% æˆ– æ—¶é—´åç§» >= 1å°æ—¶ï¼‰
        for aligned in comparison['aligned_stages']:
            has_load_diff = abs(aligned['load_difference_percent']) > 20
            has_time_shift = abs(aligned['time_shift']) >= 1.0  # åç§»è¶…è¿‡1å°æ—¶
            
            if has_load_diff or has_time_shift:
                diff_info = {
                    'current_stage': aligned['current_stage'],
                    'historical_stage': aligned['historical_stage'],
                    'time_range': aligned['current_time_range'],
                    'historical_time_range': aligned['historical_time_range'],
                    'load_change': aligned['load_difference'],
                    'load_change_percent': aligned['load_difference_percent'],
                    'time_shift': aligned['time_shift'],
                    'change_type': 'å¢åŠ ' if aligned['load_difference'] > 0 else 'å‡å°‘',
                    'shift_direction': 'å³ç§»(æ¨è¿Ÿ)' if aligned['time_shift'] > 0 else ('å·¦ç§»(æå‰)' if aligned['time_shift'] < 0 else 'æ— åç§»'),
                    'explanations': []
                }
                
                # 4. ç»“åˆäººçš„è¡Œä¸ºå¯¹å·®å¼‚è¿›è¡Œè§£é‡Š
                curr_start_hour = float(aligned['current_time_range'].split('-')[0].replace('h', ''))
                hist_start_hour = float(aligned['historical_time_range'].split('-')[0].replace('h', ''))
                
                # é¦–å…ˆè§£é‡Šæ—¶é—´åç§»ï¼ˆé˜¶æ®µçš„å·¦ç§»æˆ–å³ç§»ï¼‰
                if abs(aligned['time_shift']) >= 1.0:
                    shift_hours = abs(aligned['time_shift'])
                    shift_dir = 'æ¨è¿Ÿ' if aligned['time_shift'] > 0 else 'æå‰'
                    
                    # åˆ¤æ–­é˜¶æ®µç±»å‹ä»¥æä¾›æ›´å…·ä½“çš„è§£é‡Š
                    if 6 <= hist_start_hour < 9 or 6 <= curr_start_hour < 9:
                        diff_info['explanations'].append(
                            f'æ—©é«˜å³°é˜¶æ®µæ—¶é—´{shift_dir}çº¦{shift_hours:.1f}å°æ—¶ï¼Œå¯èƒ½æ˜¯ï¼šå› ä¸ºå‘¨æœ«/å‡æ—¥å¯¼è‡´èµ·åºŠæ—¶é—´{shift_dir}ã€æˆ–ä½œæ¯æ—¶é—´è°ƒæ•´'
                        )
                    elif 12 <= hist_start_hour < 14 or 12 <= curr_start_hour < 14:
                        diff_info['explanations'].append(
                            f'åˆé—´é˜¶æ®µæ—¶é—´{shift_dir}çº¦{shift_hours:.1f}å°æ—¶ï¼Œå¯èƒ½æ˜¯ï¼šç”¨é¤æ—¶é—´{shift_dir}ã€æˆ–åˆä¼‘ä¹ æƒ¯æ”¹å˜'
                        )
                    elif 18 <= hist_start_hour < 22 or 18 <= curr_start_hour < 22:
                        diff_info['explanations'].append(
                            f'æ™šé«˜å³°é˜¶æ®µæ—¶é—´{shift_dir}çº¦{shift_hours:.1f}å°æ—¶ï¼Œå¯èƒ½æ˜¯ï¼šä¸‹ç­/å›å®¶æ—¶é—´{shift_dir}ã€æˆ–æ™šé¤æ—¶é—´è°ƒæ•´'
                        )
                    elif 22 <= hist_start_hour or 22 <= curr_start_hour or curr_start_hour < 6 or hist_start_hour < 6:
                        diff_info['explanations'].append(
                            f'å¤œé—´é˜¶æ®µæ—¶é—´{shift_dir}çº¦{shift_hours:.1f}å°æ—¶ï¼Œå¯èƒ½æ˜¯ï¼šå°±å¯æ—¶é—´{shift_dir}ã€æˆ–å¤œé—´æ´»åŠ¨ä¹ æƒ¯æ”¹å˜'
                        )
                    else:
                        diff_info['explanations'].append(
                            f'è¯¥é˜¶æ®µæ—¶é—´æ•´ä½“{shift_dir}çº¦{shift_hours:.1f}å°æ—¶ï¼Œå¯èƒ½æ˜¯ï¼šæ—¥å¸¸ä½œæ¯æ—¶é—´è°ƒæ•´ã€å·¥ä½œ/ä¼‘æ¯æ¨¡å¼æ”¹å˜'
                        )
                
                # åŸºäºæ—¶é—´æ®µå’Œè´Ÿè·å˜åŒ–çš„è¡Œä¸ºè§£é‡Šï¼ˆä»…åœ¨è´Ÿè·å·®å¼‚æ˜¾è‘—æ—¶æ·»åŠ ï¼‰
                if abs(aligned['load_difference_percent']) > 20:
                    if aligned['load_difference'] > 0:  # è´Ÿè·å¢åŠ 
                        if 6 <= curr_start_hour < 9:
                            diff_info['explanations'].append(
                                'æ—©é«˜å³°æ—¶æ®µè´Ÿè·å¢åŠ ï¼Œå¯èƒ½æ˜¯ï¼šèµ·åºŠæ—¶é—´æå‰ã€æ—©é¤å‡†å¤‡æ›´å¤æ‚ã€æˆ–å¢åŠ äº†çƒ­æ°´å™¨/å’–å•¡æœºä½¿ç”¨'
                            )
                        elif 9 <= curr_start_hour < 12:
                            diff_info['explanations'].append(
                                'ä¸Šåˆæ—¶æ®µè´Ÿè·å¢åŠ ï¼Œå¯èƒ½æ˜¯ï¼šåœ¨å®¶åŠå…¬ã€ä½¿ç”¨æ›´å¤šç”µå™¨ã€æˆ–å®¶åº­æˆå‘˜æœªå¤–å‡º'
                            )
                        elif 12 <= curr_start_hour < 14:
                            diff_info['explanations'].append(
                                'åˆé—´æ—¶æ®µè´Ÿè·å¢åŠ ï¼Œå¯èƒ½æ˜¯ï¼šåœ¨å®¶ç”¨é¤ã€ä½¿ç”¨å¨æˆ¿ç”µå™¨å¢åŠ ã€æˆ–åˆä¼‘æœŸé—´ä½¿ç”¨ç©ºè°ƒ/æš–æ°”'
                            )
                        elif 14 <= curr_start_hour < 18:
                            diff_info['explanations'].append(
                                'ä¸‹åˆæ—¶æ®µè´Ÿè·å¢åŠ ï¼Œå¯èƒ½æ˜¯ï¼šåœ¨å®¶æ—¶é—´å¢åŠ ã€ä½¿ç”¨å¨±ä¹è®¾å¤‡ã€æˆ–æå‰å‡†å¤‡æ™šé¤'
                            )
                        elif 18 <= curr_start_hour < 22:
                            diff_info['explanations'].append(
                                'æ™šé«˜å³°æ—¶æ®µè´Ÿè·å¢åŠ ï¼Œå¯èƒ½æ˜¯ï¼šå›å®¶æ—¶é—´æå‰ã€æ™šé¤å‡†å¤‡æ›´å¤æ‚ã€å®¶åº­å¨±ä¹æ´»åŠ¨å¢åŠ ã€æˆ–ä½¿ç”¨æ›´å¤šç…§æ˜å’Œç©ºè°ƒ'
                            )
                        else:  # å¤œé—´
                            diff_info['explanations'].append(
                                'å¤œé—´æ—¶æ®µè´Ÿè·å¢åŠ ï¼Œå¯èƒ½æ˜¯ï¼šå°±å¯æ—¶é—´æ¨è¿Ÿã€å¤œé—´ä½¿ç”¨ç”µå™¨å¢åŠ ã€æˆ–ä¿æŒæ›´å¤šè®¾å¤‡å¾…æœº'
                            )
                    else:  # è´Ÿè·å‡å°‘
                        if 6 <= curr_start_hour < 9:
                            diff_info['explanations'].append(
                                'æ—©é«˜å³°æ—¶æ®µè´Ÿè·å‡å°‘ï¼Œå¯èƒ½æ˜¯ï¼šå¤–å‡ºæ—¶é—´æå‰ã€ç®€åŒ–æ—©é¤å‡†å¤‡ã€æˆ–å‡å°‘ç”µå™¨ä½¿ç”¨'
                            )
                        elif 9 <= curr_start_hour < 12:
                            diff_info['explanations'].append(
                                'ä¸Šåˆæ—¶æ®µè´Ÿè·å‡å°‘ï¼Œå¯èƒ½æ˜¯ï¼šå®¶åº­æˆå‘˜å¤–å‡ºå¢åŠ ã€å‡å°‘åœ¨å®¶åŠå…¬ã€æˆ–ä¼˜åŒ–äº†ç”µå™¨ä½¿ç”¨'
                            )
                        elif 12 <= curr_start_hour < 14:
                            diff_info['explanations'].append(
                                'åˆé—´æ—¶æ®µè´Ÿè·å‡å°‘ï¼Œå¯èƒ½æ˜¯ï¼šå¤–å‡ºç”¨é¤ã€å‡å°‘å¨æˆ¿ç”µå™¨ä½¿ç”¨ã€æˆ–ä¼˜åŒ–äº†ç©ºè°ƒä½¿ç”¨'
                            )
                        elif 14 <= curr_start_hour < 18:
                            diff_info['explanations'].append(
                                'ä¸‹åˆæ—¶æ®µè´Ÿè·å‡å°‘ï¼Œå¯èƒ½æ˜¯ï¼šå¤–å‡ºæ—¶é—´å»¶é•¿ã€å‡å°‘ç”µå™¨å¾…æœºã€æˆ–æ”¹å–„äº†èŠ‚èƒ½ä¹ æƒ¯'
                            )
                        elif 18 <= curr_start_hour < 22:
                            diff_info['explanations'].append(
                                'æ™šé«˜å³°æ—¶æ®µè´Ÿè·å‡å°‘ï¼Œå¯èƒ½æ˜¯ï¼šå›å®¶æ—¶é—´æ¨è¿Ÿã€ç®€åŒ–æ™šé¤å‡†å¤‡ã€å‡å°‘å¨±ä¹è®¾å¤‡ä½¿ç”¨ã€æˆ–æ”¹å–„ç…§æ˜å’Œç©ºè°ƒä½¿ç”¨ä¹ æƒ¯'
                            )
                        else:  # å¤œé—´
                            diff_info['explanations'].append(
                                'å¤œé—´æ—¶æ®µè´Ÿè·å‡å°‘ï¼Œå¯èƒ½æ˜¯ï¼šå°±å¯æ—¶é—´æå‰ã€å…³é—­æ›´å¤šç”µå™¨ã€æˆ–å‡å°‘è®¾å¤‡å¾…æœºåŠŸè€—'
                            )
                
                # ç»“åˆç¯å¢ƒç‰¹å¾çš„è§£é‡Š
                if 'environment_diff' in aligned and aligned['environment_diff']:
                    env_diff = aligned['environment_diff']
                    
                    if 'temperature_current' in env_diff:
                        temp_diff = env_diff['temperature_current']['diff']
                        if abs(temp_diff) > 5:
                            if temp_diff > 0:
                                diff_info['explanations'].append(
                                    f'ç¯å¢ƒæ¸©åº¦å‡é«˜{abs(temp_diff):.1f}Â°Cï¼Œå¯èƒ½å¢åŠ ç©ºè°ƒåˆ¶å†·éœ€æ±‚'
                                )
                            else:
                                diff_info['explanations'].append(
                                    f'ç¯å¢ƒæ¸©åº¦é™ä½{abs(temp_diff):.1f}Â°Cï¼Œå¯èƒ½å¢åŠ ä¾›æš–éœ€æ±‚'
                                )
                    
                    if 'humidity_current' in env_diff:
                        humid_diff = env_diff['humidity_current']['diff']
                        if abs(humid_diff) > 15:
                            if humid_diff > 0:
                                diff_info['explanations'].append(
                                    f'æ¹¿åº¦å¢åŠ {abs(humid_diff):.1f}%ï¼Œå¯èƒ½å½±å“é™¤æ¹¿è®¾å¤‡ä½¿ç”¨'
                                )
                            else:
                                diff_info['explanations'].append(
                                    f'æ¹¿åº¦é™ä½{abs(humid_diff):.1f}%ï¼Œå¯èƒ½å‡å°‘é™¤æ¹¿éœ€æ±‚'
                                )
                    
                    if 'cloudCover_current' in env_diff:
                        cloud_diff = env_diff['cloudCover_current']['diff']
                        if abs(cloud_diff) > 0.3:
                            if cloud_diff > 0:
                                diff_info['explanations'].append(
                                    'äº‘é‡å¢åŠ ï¼Œè‡ªç„¶é‡‡å…‰å‡å°‘ï¼Œå¯èƒ½å¢åŠ ç…§æ˜è´Ÿè·'
                                )
                            else:
                                diff_info['explanations'].append(
                                    'äº‘é‡å‡å°‘ï¼Œè‡ªç„¶é‡‡å…‰å¢åŠ ï¼Œå¯èƒ½å‡å°‘ç…§æ˜è´Ÿè·'
                                )
                
                comparison['significant_differences'].append(diff_info)
        
        # 5. ç”Ÿæˆæ€»ä½“è¡Œä¸ºè§£é‡Š
        if comparison['significant_differences']:
            comparison['behavior_explanations'].append(
                f"å…±è¯†åˆ«å‡º{len(comparison['significant_differences'])}ä¸ªå·®å¼‚æ˜¾è‘—çš„è´Ÿè·é˜¶æ®µ"
            )
            
            # ç»Ÿè®¡å¢åŠ å’Œå‡å°‘çš„é˜¶æ®µæ•°
            increase_count = sum(1 for d in comparison['significant_differences'] if d['load_change'] > 0)
            decrease_count = len(comparison['significant_differences']) - increase_count
            
            # ç»Ÿè®¡æ—¶é—´åç§»æ¨¡å¼
            shift_count = sum(1 for d in comparison['significant_differences'] if abs(d.get('time_shift', 0)) >= 1.0)
            right_shift_count = sum(1 for d in comparison['significant_differences'] if d.get('time_shift', 0) >= 1.0)
            left_shift_count = sum(1 for d in comparison['significant_differences'] if d.get('time_shift', 0) <= -1.0)
            
            # æ·»åŠ æ—¶é—´åç§»æ€»ä½“åˆ†æ
            if shift_count > 0:
                if right_shift_count > left_shift_count:
                    comparison['behavior_explanations'].append(
                        f'æ—¶é—´åç§»æ¨¡å¼ï¼šæ•´ä½“å³ç§»(æ¨è¿Ÿ)ä¸ºä¸»ï¼Œ{shift_count}ä¸ªé˜¶æ®µæœ‰æ˜¾è‘—æ—¶é—´åç§»ï¼ˆ{right_shift_count}ä¸ªå³ç§»ï¼Œ{left_shift_count}ä¸ªå·¦ç§»ï¼‰'
                    )
                    comparison['behavior_explanations'].append(
                        'åç§»åŸå› ï¼šå¯èƒ½æ˜¯å‘¨æœ«/å‡æ—¥ä½œæ¯æ¨è¿Ÿã€å·¥ä½œæ—¶é—´è°ƒæ•´ã€æˆ–ç”Ÿæ´»ä¹ æƒ¯æ”¹å˜'
                    )
                elif left_shift_count > right_shift_count:
                    comparison['behavior_explanations'].append(
                        f'æ—¶é—´åç§»æ¨¡å¼ï¼šæ•´ä½“å·¦ç§»(æå‰)ä¸ºä¸»ï¼Œ{shift_count}ä¸ªé˜¶æ®µæœ‰æ˜¾è‘—æ—¶é—´åç§»ï¼ˆ{left_shift_count}ä¸ªå·¦ç§»ï¼Œ{right_shift_count}ä¸ªå³ç§»ï¼‰'
                    )
                    comparison['behavior_explanations'].append(
                        'åç§»åŸå› ï¼šå¯èƒ½æ˜¯å·¥ä½œæ—¥ä½œæ¯æå‰ã€æ—©èµ·ä¹ æƒ¯å…»æˆã€æˆ–æ´»åŠ¨æ—¶é—´æ•´ä½“å‰ç§»'
                    )
                else:
                    comparison['behavior_explanations'].append(
                        f'æ—¶é—´åç§»æ¨¡å¼ï¼šå·¦ç§»å’Œå³ç§»å¹¶å­˜ï¼Œ{shift_count}ä¸ªé˜¶æ®µæœ‰æ˜¾è‘—æ—¶é—´åç§»'
                    )
                    comparison['behavior_explanations'].append(
                        'åç§»åŸå› ï¼šä¸åŒæ—¶æ®µçš„æ´»åŠ¨æ—¶é—´è°ƒæ•´ï¼Œç”¨ç”µæ¨¡å¼å‘ç”Ÿé‡ç»„'
                    )
            
            # æ·»åŠ è´Ÿè·å˜åŒ–è¶‹åŠ¿åˆ†æ
            if increase_count > decrease_count:
                comparison['behavior_explanations'].append(
                    f'æ•´ä½“è¶‹åŠ¿ï¼šè´Ÿè·å¢åŠ ä¸ºä¸»({increase_count}ä¸ªé˜¶æ®µå¢åŠ ï¼Œ{decrease_count}ä¸ªé˜¶æ®µå‡å°‘)'
                )
                comparison['behavior_explanations'].append(
                    'å¯èƒ½åŸå› ï¼šå®¶åº­æ´»åŠ¨å¢åŠ ã€åœ¨å®¶æ—¶é—´å»¶é•¿ã€æ–°å¢ç”¨ç”µè®¾å¤‡ã€æˆ–å­£èŠ‚æ€§ç”¨ç”µéœ€æ±‚å˜åŒ–'
                )
            elif decrease_count > increase_count:
                comparison['behavior_explanations'].append(
                    f'æ•´ä½“è¶‹åŠ¿ï¼šè´Ÿè·å‡å°‘ä¸ºä¸»({decrease_count}ä¸ªé˜¶æ®µå‡å°‘ï¼Œ{increase_count}ä¸ªé˜¶æ®µå¢åŠ )'
                )
                comparison['behavior_explanations'].append(
                    'å¯èƒ½åŸå› ï¼šå¤–å‡ºæ—¶é—´å¢åŠ ã€å‡å°‘ç”µå™¨ä½¿ç”¨ã€èŠ‚èƒ½ä¹ æƒ¯æ”¹å–„ã€æˆ–å­£èŠ‚æ€§ç”¨ç”µéœ€æ±‚é™ä½'
                )
            else:
                comparison['behavior_explanations'].append(
                    f'æ•´ä½“è¶‹åŠ¿ï¼šå¢å‡å¹³è¡¡({increase_count}ä¸ªé˜¶æ®µå¢åŠ ï¼Œ{decrease_count}ä¸ªé˜¶æ®µå‡å°‘)'
                )
                comparison['behavior_explanations'].append(
                    'å¯èƒ½åŸå› ï¼šç”¨ç”µæ¨¡å¼è°ƒæ•´ï¼Œä¸åŒæ—¶æ®µçš„ç”¨ç”µè¡Œä¸ºå‘ç”Ÿäº†å˜åŒ–'
                )
        else:
            comparison['behavior_explanations'].append(
                'å„é˜¶æ®µè´Ÿè·å·®å¼‚è¾ƒå°ï¼Œç”¨ç”µæ¨¡å¼ä¿æŒç›¸å¯¹ç¨³å®š'
            )
        
        return comparison
        
    except Exception as e:
        print(f"âŒ å†å²è´Ÿè·å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'stage_count_comparison': {},
            'aligned_stages': [],
            'significant_differences': [],
            'behavior_explanations': [],
            'error': str(e)
        }

def compare_predicted_with_multiple_historical_stages(predicted_segments, historical_data_dict,
                                                      predicted_feat_df, predicted_times, predicted_load,
                                                      comparison_days=[1, 3, 7]):
    """
    å°†é¢„æµ‹æ—¥è´Ÿè·ä¸å¤šä¸ªå†å²è´Ÿè·ï¼ˆ7/3/1å¤©å‰ï¼‰è¿›è¡Œå¯¹æ¯”åˆ†æ
    
    å‚æ•°:
    - predicted_segments: é¢„æµ‹æ—¥è´Ÿè·åˆ†æ®µä¿¡æ¯ [(start_idx, end_idx, state, mean_load), ...]
    - historical_data_dict: å†å²æ•°æ®å­—å…¸ï¼Œæ ¼å¼ä¸º {days_ago: {'segments': [...], 'feat_df': df, 'times': [...], 'load': [...]}}
    - predicted_feat_df: é¢„æµ‹æ—¥ç‰¹å¾æ•°æ®æ¡†
    - predicted_times: é¢„æµ‹æ—¥æ—¶é—´ç‚¹åˆ—è¡¨
    - predicted_load: é¢„æµ‹æ—¥è´Ÿè·å€¼æ•°ç»„
    - comparison_days: è¦å¯¹æ¯”çš„å†å²å¤©æ•°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[1, 3, 7]å¤©å‰
    
    è¿”å›:
    - multi_comparison: åŒ…å«ä¸å¤šä¸ªå†å²æ—¶æœŸå¯¹æ¯”åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        multi_comparison = {
            'comparison_days': comparison_days,
            'comparisons': {},
            'summary': {
                'stage_count_trends': [],
                'load_trends': [],
                'time_shift_trends': [],
                'behavior_patterns': []
            }
        }
        
        # å¯¹æ¯ä¸ªå†å²æ—¶æœŸè¿›è¡Œå¯¹æ¯”
        for days_ago in comparison_days:
            if days_ago not in historical_data_dict:
                print(f"âš ï¸ {days_ago}å¤©å‰çš„å†å²æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            hist_data = historical_data_dict[days_ago]
            
            # è°ƒç”¨å•æ¬¡å¯¹æ¯”å‡½æ•°
            comparison = compare_with_historical_stages(
                predicted_segments, hist_data['segments'],
                predicted_feat_df, hist_data['feat_df'],
                predicted_times, hist_data['times'],
                predicted_load, hist_data['load']
            )
            
            multi_comparison['comparisons'][days_ago] = comparison
        
        # ç”Ÿæˆè·¨æ—¶æœŸçš„è¶‹åŠ¿æ€»ç»“
        if len(multi_comparison['comparisons']) > 0:
            # 1. é˜¶æ®µæ•°é‡å˜åŒ–è¶‹åŠ¿
            stage_counts = []
            for days_ago in sorted(multi_comparison['comparisons'].keys()):
                comp = multi_comparison['comparisons'][days_ago]
                scc = comp.get('stage_count_comparison', {})
                stage_counts.append({
                    'days_ago': days_ago,
                    'predicted_count': scc.get('current_count', 0),
                    'historical_count': scc.get('historical_count', 0),
                    'change': scc.get('change', 0)
                })
            
            multi_comparison['summary']['stage_count_trends'] = stage_counts
            
            # åˆ†æé˜¶æ®µæ•°é‡å˜åŒ–è¶‹åŠ¿
            if len(stage_counts) > 1:
                changes = [sc['change'] for sc in stage_counts]
                if all(c > 0 for c in changes):
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸è¿‡å»{comparison_days}å¤©ç›¸æ¯”ï¼Œè´Ÿè·é˜¶æ®µæ•°æŒç»­å¢åŠ ï¼Œè¯´æ˜ç”¨ç”µè¡Œä¸ºé€æ¸å¤šæ ·åŒ–å’Œå¤æ‚åŒ–'
                    )
                elif all(c < 0 for c in changes):
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸è¿‡å»{comparison_days}å¤©ç›¸æ¯”ï¼Œè´Ÿè·é˜¶æ®µæ•°æŒç»­å‡å°‘ï¼Œè¯´æ˜ç”¨ç”µè¡Œä¸ºé€æ¸è§„å¾‹åŒ–å’Œç®€åŒ–'
                    )
                else:
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸è¿‡å»{comparison_days}å¤©ç›¸æ¯”ï¼Œè´Ÿè·é˜¶æ®µæ•°å˜åŒ–ä¸ä¸€è‡´ï¼Œè¯´æ˜ç”¨ç”µæ¨¡å¼å­˜åœ¨æ³¢åŠ¨'
                    )
            
            # 2. è´Ÿè·å˜åŒ–è¶‹åŠ¿åˆ†æ
            for days_ago in sorted(multi_comparison['comparisons'].keys()):
                comp = multi_comparison['comparisons'][days_ago]
                sig_diffs = comp.get('significant_differences', [])
                
                if sig_diffs:
                    increase_count = sum(1 for d in sig_diffs if d['load_change'] > 0)
                    decrease_count = len(sig_diffs) - increase_count
                    
                    multi_comparison['summary']['load_trends'].append({
                        'days_ago': days_ago,
                        'increase_count': increase_count,
                        'decrease_count': decrease_count,
                        'total_significant': len(sig_diffs)
                    })
            
            # 3. æ—¶é—´åç§»è¶‹åŠ¿åˆ†æ
            for days_ago in sorted(multi_comparison['comparisons'].keys()):
                comp = multi_comparison['comparisons'][days_ago]
                sig_diffs = comp.get('significant_differences', [])
                
                if sig_diffs:
                    right_shift_count = sum(1 for d in sig_diffs if d.get('time_shift', 0) >= 1.0)
                    left_shift_count = sum(1 for d in sig_diffs if d.get('time_shift', 0) <= -1.0)
                    shift_count = right_shift_count + left_shift_count
                    
                    if shift_count > 0:
                        multi_comparison['summary']['time_shift_trends'].append({
                            'days_ago': days_ago,
                            'shift_count': shift_count,
                            'right_shift_count': right_shift_count,
                            'left_shift_count': left_shift_count,
                            'dominant_direction': 'å³ç§»(æ¨è¿Ÿ)' if right_shift_count > left_shift_count else ('å·¦ç§»(æå‰)' if left_shift_count > right_shift_count else 'æ··åˆ')
                        })
            
            # 4. ç”Ÿæˆç»¼åˆè¡Œä¸ºæ¨¡å¼è§£é‡Š
            if multi_comparison['summary']['time_shift_trends']:
                # åˆ†ææ—¶é—´åç§»çš„ä¸€è‡´æ€§
                shift_trends = multi_comparison['summary']['time_shift_trends']
                dominant_dirs = [st['dominant_direction'] for st in shift_trends]
                
                if all('å³ç§»' in d for d in dominant_dirs):
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸è¿‡å»{comparison_days}å¤©ç›¸æ¯”ï¼Œè´Ÿè·é˜¶æ®µæŒç»­å³ç§»(æ¨è¿Ÿ)ï¼Œè¯´æ˜ä½œæ¯æ—¶é—´é€æ¸æ¨è¿Ÿï¼Œå¯èƒ½æ˜¯å‘¨æœ«/å‡æ—¥æ•ˆåº”ã€æˆ–ç”Ÿæ´»ä¹ æƒ¯æ”¹å˜'
                    )
                elif all('å·¦ç§»' in d for d in dominant_dirs):
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸è¿‡å»{comparison_days}å¤©ç›¸æ¯”ï¼Œè´Ÿè·é˜¶æ®µæŒç»­å·¦ç§»(æå‰)ï¼Œè¯´æ˜ä½œæ¯æ—¶é—´é€æ¸æå‰ï¼Œå¯èƒ½æ˜¯å·¥ä½œæ—¥æ•ˆåº”ã€æˆ–æ—©èµ·ä¹ æƒ¯å…»æˆ'
                    )
                else:
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸è¿‡å»{comparison_days}å¤©ç›¸æ¯”ï¼Œè´Ÿè·é˜¶æ®µæ—¶é—´åç§»æ–¹å‘ä¸ä¸€è‡´ï¼Œè¯´æ˜ä½œæ¯æ—¶é—´æœ‰æ³¢åŠ¨ï¼Œç”¨ç”µæ¨¡å¼ä¸ç¨³å®š'
                    )
            
            # 5. è´Ÿè·æ°´å¹³å˜åŒ–è¶‹åŠ¿
            if multi_comparison['summary']['load_trends']:
                load_trends = multi_comparison['summary']['load_trends']
                
                # è®¡ç®—æ•´ä½“è´Ÿè·å¢å‡è¶‹åŠ¿
                total_increases = sum(lt['increase_count'] for lt in load_trends)
                total_decreases = sum(lt['decrease_count'] for lt in load_trends)
                
                if total_increases > total_decreases * 1.5:
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸å†å²æ—¶æœŸç›¸æ¯”ï¼Œè´Ÿè·æ•´ä½“å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¯èƒ½åŸå› ï¼šåœ¨å®¶æ—¶é—´å¢åŠ ã€æ–°å¢ç”¨ç”µè®¾å¤‡ã€å­£èŠ‚æ€§éœ€æ±‚ä¸Šå‡'
                    )
                elif total_decreases > total_increases * 1.5:
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸å†å²æ—¶æœŸç›¸æ¯”ï¼Œè´Ÿè·æ•´ä½“å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå¯èƒ½åŸå› ï¼šå¤–å‡ºæ—¶é—´å¢åŠ ã€èŠ‚èƒ½æªæ–½å®æ–½ã€å­£èŠ‚æ€§éœ€æ±‚ä¸‹é™'
                    )
                else:
                    multi_comparison['summary']['behavior_patterns'].append(
                        f'ä¸å†å²æ—¶æœŸç›¸æ¯”ï¼Œè´Ÿè·å¢å‡è¾ƒä¸ºå¹³è¡¡ï¼Œç”¨ç”µæ°´å¹³ç›¸å¯¹ç¨³å®š'
                    )
        
        return multi_comparison
        
    except Exception as e:
        print(f"âŒ å¤šå†å²æ—¶æœŸå¯¹æ¯”åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'comparison_days': comparison_days,
            'comparisons': {},
            'summary': {},
            'error': str(e)
        }

def generate_explanation_report(explanations, output_path):
    """
    ç”Ÿæˆè´Ÿè·å˜åŒ–å¯è§£é‡Šæ€§æŠ¥å‘Šï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    
    å‚æ•°:
    - explanations: è§£é‡Šåˆ†æç»“æœå­—å…¸
    - output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("è´Ÿè·å˜åŒ–å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            # 1. é˜¶æ®µåˆ†æ
            f.write("ã€é˜¶æ®µè¯¦ç»†åˆ†æã€‘\n")
            f.write("-"*80 + "\n")
            for seg in explanations.get('segment_analysis', []):
                f.write(f"\né˜¶æ®µ {seg['segment_id']}:\n")
                f.write(f"  æ—¶é—´èŒƒå›´: {seg['start_time']:.2f}h - {seg['end_time']:.2f}h (æŒç»­ {seg['duration_hours']:.2f}å°æ—¶)\n")
                f.write(f"  è´Ÿè·æ°´å¹³: {seg['load_level']} (å¹³å‡å€¼: {seg['mean_load']:.4f})\n")
                f.write(f"  çŠ¶æ€ç¼–å·: {seg['state']}\n")
                f.write(f"  å…³é”®å½±å“å› ç´ :\n")
                for factor in seg['key_factors']:
                    f.write(f"    â€¢ {factor}\n")
            
            # 2. è¶‹åŠ¿åˆ†æ
            if explanations.get('trend_analysis'):
                f.write("\n\nã€é˜¶æ®µé—´è¶‹åŠ¿å˜åŒ–åˆ†æã€‘\n")
                f.write("-"*80 + "\n")
                trend = explanations['trend_analysis']
                f.write(f"æ€»é˜¶æ®µæ•°: {trend.get('total_segments', 0)}\n")
                f.write(f"è´Ÿè·èŒƒå›´: {trend.get('min_load', 0):.4f} - {trend.get('max_load', 0):.4f}\n")
                f.write(f"è´Ÿè·æ³¢åŠ¨å¹…åº¦: {trend.get('load_range', 0):.4f}\n\n")
                
                for trans in trend.get('transitions', []):
                    f.write(f"\né˜¶æ®µ {trans['from_segment']} â†’ é˜¶æ®µ {trans['to_segment']}:\n")
                    f.write(f"  å˜åŒ–è¶‹åŠ¿: {trans['trend']}\n")
                    f.write(f"  è´Ÿè·å˜åŒ–: {trans['load_change']:+.4f} ({trans['load_change_percent']:+.1f}%)\n")
                    f.write(f"  å˜åŒ–åŸå› :\n")
                    for exp in trans['explanation']:
                        f.write(f"    â€¢ {exp}\n")
            
            # 3. ç‰¹å¾é‡è¦æ€§
            if explanations.get('feature_importance'):
                f.write("\n\nã€ç‰¹å¾é‡è¦æ€§åˆ†æã€‘\n")
                f.write("-"*80 + "\n")
                feat_imp = explanations['feature_importance']
                
                if feat_imp.get('top_features'):
                    f.write("æœ€é‡è¦çš„ç¯å¢ƒç‰¹å¾:\n")
                    for feat in feat_imp['top_features']:
                        corr = feat_imp['correlations'].get(feat, 0)
                        f.write(f"  â€¢ {feat} (ç›¸å…³ç³»æ•°: {corr:+.3f})\n")
                
                if feat_imp.get('interpretation'):
                    f.write("\nç‰¹å¾å½±å“è§£é‡Š:\n")
                    for interp in feat_imp['interpretation']:
                        f.write(f"  â€¢ {interp}\n")
            
            # 4. ç¯å¢ƒå› ç´ å½±å“
            if explanations.get('environmental_impact'):
                f.write("\n\nã€ç¯å¢ƒå› ç´ ç»¼åˆå½±å“ã€‘\n")
                f.write("-"*80 + "\n")
                for feat, stats in explanations['environmental_impact'].items():
                    f.write(f"\n{feat}:\n")
                    f.write(f"  å¹³å‡å€¼: {stats['mean']:.2f}\n")
                    f.write(f"  æ ‡å‡†å·®: {stats['std']:.2f}\n")
                    f.write(f"  èŒƒå›´: {stats['min']:.2f} - {stats['max']:.2f}\n")
                    f.write(f"  æ³¢åŠ¨å¹…åº¦: {stats['range']:.2f}\n")
            
            # 5. å†å²å¯¹æ¯”åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
            if explanations.get('historical_comparison'):
                comp = explanations['historical_comparison']
                f.write("\n\nã€ä¸å†å²è´Ÿè·å¯¹æ¯”åˆ†æã€‘\n")
                f.write("="*80 + "\n")
                
                # 5.1 é˜¶æ®µæ•°é‡å¯¹æ¯”
                if comp.get('stage_count_comparison'):
                    scc = comp['stage_count_comparison']
                    f.write("\nâ–¶ é˜¶æ®µæ•°é‡å˜åŒ–åˆ†æ\n")
                    f.write("-"*80 + "\n")
                    f.write(f"å½“å‰é˜¶æ®µæ•°: {scc['current_count']}\n")
                    f.write(f"å†å²é˜¶æ®µæ•°: {scc['historical_count']}\n")
                    f.write(f"å˜åŒ–: {scc['change']:+d} ä¸ªé˜¶æ®µ ({scc['change_percent']:+.1f}%)\n")
                    f.write(f"è¶‹åŠ¿: {scc['trend']}\n\n")
                    f.write("åŸå› åˆ†æ:\n")
                    for reason in scc.get('reasons', []):
                        f.write(f"  {reason}\n")
                
                # 5.2 é€é˜¶æ®µå¯¹é½åˆ†æ
                if comp.get('aligned_stages'):
                    f.write("\n\nâ–¶ é€é˜¶æ®µå¯¹é½åˆ†æ\n")
                    f.write("-"*80 + "\n")
                    for aligned in comp['aligned_stages']:
                        f.write(f"\nå½“å‰é˜¶æ®µ{aligned['current_stage']} â†” å†å²é˜¶æ®µ{aligned['historical_stage']}:\n")
                        f.write(f"  æ—¶é—´èŒƒå›´: {aligned['current_time_range']} (å½“å‰) vs {aligned['historical_time_range']} (å†å²)\n")
                        
                        # æ·»åŠ æ—¶é—´åç§»ä¿¡æ¯
                        if 'time_shift' in aligned:
                            time_shift = aligned['time_shift']
                            if abs(time_shift) >= 1.0:
                                shift_dir = 'å³ç§»(æ¨è¿Ÿ)' if time_shift > 0 else 'å·¦ç§»(æå‰)'
                                f.write(f"  æ—¶é—´åç§»: {abs(time_shift):.1f} å°æ—¶ ({shift_dir})\n")
                        
                        f.write(f"  è´Ÿè·æ°´å¹³: {aligned['current_load']:.4f} kW (å½“å‰) vs {aligned['historical_load']:.4f} kW (å†å²)\n")
                        f.write(f"  è´Ÿè·å·®å¼‚: {aligned['load_difference']:+.4f} kW ({aligned['load_difference_percent']:+.1f}%)\n")
                        if aligned.get('environment_diff'):
                            f.write(f"  ç¯å¢ƒç‰¹å¾å·®å¼‚:\n")
                            for feat, diff in aligned['environment_diff'].items():
                                feat_name = feat.replace('_current', '')
                                f.write(f"    â€¢ {feat_name}: {diff['current']:.2f} (å½“å‰) vs {diff['historical']:.2f} (å†å²), å·®å¼‚: {diff['diff']:+.2f}\n")
                
                # 5.3 æ˜¾è‘—å·®å¼‚é˜¶æ®µ
                if comp.get('significant_differences'):
                    f.write("\n\nâ–¶ å·®å¼‚æ˜¾è‘—çš„è´Ÿè·é˜¶æ®µ\n")
                    f.write("-"*80 + "\n")
                    for diff in comp['significant_differences']:
                        f.write(f"\né˜¶æ®µ{diff['current_stage']} (å½“å‰æ—¶é—´: {diff['time_range']}, å†å²æ—¶é—´: {diff['historical_time_range']}):\n")
                        f.write(f"  è´Ÿè·å˜åŒ–: {diff['load_change']:+.4f} kW ({diff['load_change_percent']:+.1f}%)\n")
                        f.write(f"  å˜åŒ–ç±»å‹: {diff['change_type']}\n")
                        
                        # æ·»åŠ æ—¶é—´åç§»ä¿¡æ¯
                        if 'time_shift' in diff and abs(diff['time_shift']) >= 1.0:
                            f.write(f"  æ—¶é—´åç§»: {diff['time_shift']:+.1f} å°æ—¶ ({diff['shift_direction']})\n")
                        
                        f.write(f"  è¡Œä¸ºè§£é‡Š:\n")
                        for exp in diff['explanations']:
                            f.write(f"    â€¢ {exp}\n")
                
                # 5.4 æ€»ä½“è¡Œä¸ºè§£é‡Š
                if comp.get('behavior_explanations'):
                    f.write("\n\nâ–¶ æ€»ä½“è¡Œä¸ºæ¨¡å¼åˆ†æ\n")
                    f.write("-"*80 + "\n")
                    for exp in comp['behavior_explanations']:
                        f.write(f"{exp}\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write("æŠ¥å‘Šç”Ÿæˆå®Œæˆ\n")
            f.write("="*80 + "\n")
        
        print(f"âœ… å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def visualize_explanations(explanations, output_path):
    """
    å¯è§†åŒ–è´Ÿè·å˜åŒ–è§£é‡Šç»“æœ
    
    å‚æ•°:
    - explanations: è§£é‡Šåˆ†æç»“æœå­—å…¸
    - output_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
    """
    try:
        import matplotlib.pyplot as plt
        
        # åˆ›å»ºå¤šå­å›¾å¸ƒå±€
        fig = plt.figure(figsize=(16, 12))
        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
        
        # 1. é˜¶æ®µè´Ÿè·æ°´å¹³æŸ±çŠ¶å›¾
        ax1 = fig.add_subplot(gs[0, :])
        seg_analysis = explanations.get('segment_analysis', [])
        if seg_analysis:
            seg_ids = [seg['segment_id'] for seg in seg_analysis]
            seg_loads = [seg['mean_load'] for seg in seg_analysis]
            seg_levels = [seg['load_level'] for seg in seg_analysis]
            
            colors = []
            for level in seg_levels:
                if level == 'ä½è´Ÿè·':
                    colors.append('green')
                elif level == 'ä¸­ä½è´Ÿè·':
                    colors.append('lightgreen')
                elif level == 'ä¸­é«˜è´Ÿè·':
                    colors.append('orange')
                else:
                    colors.append('red')
            
            bars = ax1.bar(seg_ids, seg_loads, color=colors, alpha=0.7, edgecolor='black')
            ax1.set_xlabel('é˜¶æ®µç¼–å·' if HAS_CJK_FONT else 'Stage ID', fontsize=12)
            ax1.set_ylabel('å¹³å‡è´Ÿè·' if HAS_CJK_FONT else 'Average Load', fontsize=12)
            ax1.set_title('å„é˜¶æ®µè´Ÿè·æ°´å¹³å¯¹æ¯”' if HAS_CJK_FONT else 'Load Level Comparison', fontsize=14, fontweight='bold')
            ax1.grid(True, alpha=0.3, axis='y')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, load, level in zip(bars, seg_loads, seg_levels):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{load:.3f}\n{level}',
                        ha='center', va='bottom', fontsize=9)
        
        # 2. è¶‹åŠ¿å˜åŒ–æŠ˜çº¿å›¾
        ax2 = fig.add_subplot(gs[1, 0])
        trend_analysis = explanations.get('trend_analysis', {})
        if trend_analysis and trend_analysis.get('transitions'):
            transitions = trend_analysis['transitions']
            from_segs = [t['from_segment'] for t in transitions]
            change_pcts = [t['load_change_percent'] for t in transitions]
            
            ax2.plot(from_segs, change_pcts, marker='o', linewidth=2, markersize=8, color='blue')
            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)
            ax2.fill_between(from_segs, 0, change_pcts, alpha=0.3, 
                            color=['green' if c < 0 else 'red' for c in change_pcts])
            ax2.set_xlabel('èµ·å§‹é˜¶æ®µ' if HAS_CJK_FONT else 'From Stage', fontsize=12)
            ax2.set_ylabel('è´Ÿè·å˜åŒ–ç‡ (%)' if HAS_CJK_FONT else 'Load Change (%)', fontsize=12)
            ax2.set_title('é˜¶æ®µé—´è´Ÿè·å˜åŒ–ç‡' if HAS_CJK_FONT else 'Load Change Rate', fontsize=12, fontweight='bold')
            ax2.grid(True, alpha=0.3)
        
        # 3. ç‰¹å¾é‡è¦æ€§æ¨ªå‘æŸ±çŠ¶å›¾
        ax3 = fig.add_subplot(gs[1, 1])
        feat_imp = explanations.get('feature_importance', {})
        if feat_imp and feat_imp.get('correlations'):
            correlations = feat_imp['correlations']
            features = list(correlations.keys())[:5]  # å–å‰5ä¸ª
            corr_values = [correlations[f] for f in features]
            
            colors = ['green' if c > 0 else 'red' for c in corr_values]
            bars = ax3.barh(features, corr_values, color=colors, alpha=0.7, edgecolor='black')
            ax3.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
            ax3.set_xlabel('ç›¸å…³ç³»æ•°' if HAS_CJK_FONT else 'Correlation', fontsize=12)
            ax3.set_ylabel('ç¯å¢ƒç‰¹å¾' if HAS_CJK_FONT else 'Features', fontsize=12)
            ax3.set_title('ç‰¹å¾ä¸è´Ÿè·ç›¸å…³æ€§' if HAS_CJK_FONT else 'Feature-Load Correlation', fontsize=12, fontweight='bold')
            ax3.grid(True, alpha=0.3, axis='x')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, corr_values):
                width = bar.get_width()
                ax3.text(width, bar.get_y() + bar.get_height()/2.,
                        f'{val:+.3f}',
                        ha='left' if width > 0 else 'right', 
                        va='center', fontsize=9)
        
        # 4. ç¯å¢ƒå› ç´ å½±å“é›·è¾¾å›¾
        ax4 = fig.add_subplot(gs[2, :], projection='polar')
        env_impact = explanations.get('environmental_impact', {})
        if env_impact:
            features = list(env_impact.keys())
            # å½’ä¸€åŒ–rangeå€¼ç”¨äºé›·è¾¾å›¾
            ranges = [env_impact[f]['range'] for f in features]
            max_range = max(ranges) if ranges else 1
            normalized_ranges = [r / max_range for r in ranges]
            
            angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False).tolist()
            normalized_ranges += normalized_ranges[:1]  # é—­åˆå›¾å½¢
            angles += angles[:1]
            
            ax4.plot(angles, normalized_ranges, 'o-', linewidth=2, color='blue')
            ax4.fill(angles, normalized_ranges, alpha=0.25, color='blue')
            ax4.set_xticks(angles[:-1])
            ax4.set_xticklabels(features, fontsize=10)
            ax4.set_ylim(0, 1)
            ax4.set_title('ç¯å¢ƒå› ç´ æ³¢åŠ¨å¹…åº¦' if HAS_CJK_FONT else 'Environmental Factors Variation', 
                         fontsize=12, fontweight='bold', pad=20)
            ax4.grid(True)
        
        plt.tight_layout()
        fig.savefig(output_path, dpi=200, bbox_inches='tight')
        plt.close(fig)
        
        print(f"âœ… å¯è§£é‡Šæ€§å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def simple_load_segmentation(load_values, n_segments=4, min_segment_length=8):
    """
    ç®€å•çš„è´Ÿè·åˆ†æ®µæ–¹æ³•ï¼ˆä½œä¸ºHMMçš„å¤‡é€‰æ–¹æ¡ˆï¼‰
    åŸºäºè´Ÿè·æ°´å¹³çš„åˆ†ä½æ•°è¿›è¡Œåˆ†æ®µï¼Œå¹¶åˆå¹¶çŸ­æ®µ
    """
    try:
        load_values = np.array(load_values)
        
        # å¯¹æ•°æ®è¿›è¡Œè½»å¾®å¹³æ»‘ï¼Œå‡å°‘å™ªå£°
        from scipy import ndimage
        smoothed_values = ndimage.median_filter(load_values.astype(float), size=3)
        
        # è®¡ç®—åˆ†ä½æ•°é˜ˆå€¼
        quantiles = np.linspace(0, 1, n_segments + 1)
        thresholds = np.quantile(smoothed_values, quantiles)
        
        # åˆ†é…çŠ¶æ€
        raw_states = np.digitize(smoothed_values, thresholds[1:-1])
        
        # åº”ç”¨æ»‘åŠ¨çª—å£å¹³æ»‘çŠ¶æ€åºåˆ—
        window_size = 5
        smoothed_states = np.zeros_like(raw_states)
        for i in range(len(raw_states)):
            start = max(0, i - window_size // 2)
            end = min(len(raw_states), i + window_size // 2 + 1)
            window = raw_states[start:end]
            # ä½¿ç”¨ä¼—æ•°ä½œä¸ºå¹³æ»‘åçš„çŠ¶æ€
            from scipy import stats
            smoothed_states[i] = int(stats.mode(window)[0])
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„å¹³å‡å€¼
        state_means = []
        for state in range(n_segments):
            state_mask = (smoothed_states == state)
            if np.any(state_mask):
                state_mean = np.mean(load_values[state_mask])
                state_means.append(state_mean)
            else:
                state_means.append(np.mean(load_values))
        
        state_means = np.array(state_means)
        
        # è¯†åˆ«åˆå§‹è¿ç»­æ®µ
        initial_segments = []
        current_state = smoothed_states[0]
        start_idx = 0
        
        for i in range(1, len(smoothed_states)):
            if smoothed_states[i] != current_state:
                end_idx = i - 1
                segment_load = np.mean(load_values[start_idx:i])
                initial_segments.append((start_idx, end_idx, current_state, segment_load))
                start_idx = i
                current_state = smoothed_states[i]
        
        # æ·»åŠ æœ€åä¸€æ®µ
        segment_load = np.mean(load_values[start_idx:])
        initial_segments.append((start_idx, len(smoothed_states) - 1, current_state, segment_load))
        
        # åˆå¹¶çŸ­æ®µ
        merged_segments = merge_short_segments(initial_segments, load_values, min_segment_length)
        
        # é‡æ–°æ„å»ºçŠ¶æ€åºåˆ—
        final_states = np.zeros_like(smoothed_states)
        for start_idx, end_idx, state, _ in merged_segments:
            final_states[start_idx:end_idx+1] = state
        
        # é‡æ–°è®¡ç®—çŠ¶æ€å¹³å‡å€¼
        final_state_means = []
        unique_states = sorted(set([seg[2] for seg in merged_segments]))
        for state in unique_states:
            state_mask = (final_states == state)
            if np.any(state_mask):
                state_mean = np.mean(load_values[state_mask])
                final_state_means.append(state_mean)
            else:
                final_state_means.append(np.mean(load_values))
        
        final_state_means = np.array(final_state_means)
        
        return final_states, final_state_means, merged_segments
        
    except Exception as e:
        print(f"âŒ ç®€å•åˆ†æ®µä¹Ÿå¤±è´¥: {e}")
        # æœ€ç®€å•çš„é€€å›æ–¹æ¡ˆ
        n = len(load_values)
        states = np.zeros(n, dtype=int)
        state_means = np.array([np.mean(load_values)])
        segments = [(0, n-1, 0, np.mean(load_values))]
        return states, state_means, segments


def plot_prediction_day(ts, dates_val, y_val, y_pred_val, out_dir, plot_date=None, step_minutes=15):
    """ç»˜åˆ¶å¹¶ä¿å­˜æŸä¸€å¤©çš„é¢„æµ‹å¯¹æ¯”å›¾ã€‚
    """
    try:
        if not dates_val:
            print("æ— éªŒè¯æ ·æœ¬ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
            return

        if plot_date is None:
            selected_date = (pd.Timestamp(dates_val[0]) + pd.Timedelta(minutes=step_minutes)).date()
        else:
            selected_date = pd.to_datetime(plot_date).date()

        # æ ¹æ®æ˜¯å¦æœ‰ä¸­æ–‡å­—ä½“ï¼Œé€‰æ‹©æ ‡ç­¾æ–‡æœ¬
        if HAS_CJK_FONT:
            label_true = 'çœŸå®ï¼ˆç›®æ ‡ä¸‹ä¸€æ—¶åˆ»ï¼‰'
            label_pred = 'é¢„æµ‹'
            label_smooth = 'åŸå§‹å¹³æ»‘ï¼ˆæ—¥æ›²çº¿ï¼‰'
            xlabel = 'æ—¶é—´æ­¥ï¼ˆç´¢å¼•ï¼‰'
            ylabel = 'è´Ÿè·'
            title_fmt = 'è´Ÿè·é¢„æµ‹å¯¹æ¯” - {}'
        else:
            label_true = 'True (target next)'
            label_pred = 'Prediction'
            label_smooth = 'Original smooth (day)'
            xlabel = 'Time step (index)'
            ylabel = 'Load'
            title_fmt = 'Load prediction comparison - {}'

        # æ„å»ºéªŒè¯é›†ä¸­æ—¶é—´->é¢„æµ‹/çœŸå®æ˜ å°„
        pred_map = {}
        true_map = {}
        for dt, true_v, pred_v in zip(dates_val, y_val, y_pred_val):
            target_time = pd.Timestamp(dt) + pd.Timedelta(minutes=step_minutes)
            key = target_time.floor('min')
            true_map[key] = true_v
            pred_map[key] = pred_v

        day_len = int(24 * 60 / step_minutes)
        base = pd.Timestamp(selected_date)
        day_index = pd.date_range(start=base, periods=day_len, freq=f"{step_minutes}T")

        true_series = pd.Series(index=list(true_map.keys()), data=list(true_map.values()))
        pred_series = pd.Series(index=list(pred_map.keys()), data=list(pred_map.values()))

        true_series = true_series[true_series.index.date == selected_date]
        pred_series = pred_series[pred_series.index.date == selected_date]

        true_full = true_series.reindex(day_index).interpolate(method='time').ffill().bfill()
        pred_full = pred_series.reindex(day_index).interpolate(method='time').ffill().bfill()

        y_true_day_full = true_full.values
        y_pred_day_full = pred_full.values

        # åŸå§‹å½“å¤©è´Ÿè·
        day_series = ts[ts.index.date == selected_date]
        if not day_series.empty:
            raw_vals = day_series['load'].values
            if len(raw_vals) != day_len:
                tmp = day_series['load'].reindex(day_index, method='nearest')
                raw_vals = tmp.values
            smooth_vals = gaussian_filter1d(raw_vals.astype(float), sigma=2.0)
        else:
            smooth_vals = None

        import matplotlib.pyplot as plt
        fig, ax = plt.subplots(figsize=(12, 6))
        time_axis = np.arange(day_len)
        ax.plot(time_axis, y_true_day_full, label=label_true, marker='o')
        ax.plot(time_axis, y_pred_day_full, label=label_pred, marker='x')

        if smooth_vals is not None and len(smooth_vals) == day_len:
            ax.plot(time_axis, smooth_vals, label=label_smooth, alpha=0.6)

        ax.set_title(title_fmt.format(selected_date.strftime('%Y-%m-%d')))
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.legend()
        plt.tight_layout()

        out_fig = os.path.join(out_dir, 'prediction_vs_true_{}.png'.format(selected_date.strftime('%Y%m%d')))
        fig.savefig(out_fig, dpi=200)
        plt.close(fig)
        print('å·²ä¿å­˜é¢„æµ‹å¯¹æ¯”å›¾: {}'.format(out_fig))
    except Exception as e:
        print('ç»˜å›¾å‡ºé”™: {}'.format(e))


def plot_all_valid_days(ts, dates_val, y_val, y_pred_val, out_dir, step_minutes=15):
    """ä¸ºéªŒè¯é›†ä¸­æ¯ä¸ªå‡ºç°ç›®æ ‡æ—¶é—´çš„æ—¥æœŸç”Ÿæˆæ—¥è´Ÿè·å¯¹æ¯”å›¾å¹¶ä¿å­˜ã€‚
    ä»…å¯¹éªŒè¯é›†ä¸­å®é™…å‡ºç°çš„æ—¥æœŸç»˜å›¾ï¼ˆè¿™äº›æ—¥æœŸå¯¹åº”å·²æœ‰å®Œæ•´å†å²çª—å£çš„æ ·æœ¬ï¼‰ã€‚
    """
    try:
        if not dates_val:
            print('æ— éªŒè¯æ ·æœ¬ï¼Œè·³è¿‡æ‰¹é‡æ—¥ç»˜å›¾ã€‚')
            return
        # ç›®æ ‡æ—¶é—´ = å½“å‰æ ·æœ¬æ—¶é—´ + step_minutes
        target_times = [pd.Timestamp(dt) + pd.Timedelta(minutes=step_minutes) for dt in dates_val]
        target_dates = pd.to_datetime(target_times).date
        unique_dates = sorted(set(target_dates))
        for ud in unique_dates:
            # æ”¶é›†å½“å¤©çš„ç´¢å¼•å’Œå€¼
            idxs = [i for i, tt in enumerate(target_times) if tt.date() == ud]
            if not idxs:
                continue
            # ä¸ºè¯¥å¤©ç”Ÿæˆå­æ•°ç»„å¹¶è°ƒç”¨ç»˜å›¾
            sub_dates = [dates_val[i] for i in idxs]
            sub_y = [y_val[i] for i in idxs]
            sub_pred = [y_pred_val[i] for i in idxs]
            try:
                plot_prediction_day(ts, sub_dates, sub_y, sub_pred, out_dir, plot_date=str(ud), step_minutes=step_minutes)
            except Exception as e:
                print('ç»˜å›¾å¤±è´¥ {}: {}'.format(ud, e))
    except Exception as e:
        print('æ‰¹é‡æ—¥ç»˜å›¾å‡ºé”™: {}'.format(e))


def visualize_results(ts, dates_val, y_val, y_pred_val, out_dir, plot_date=None, step_minutes=15):
    """ç”Ÿæˆå¹¶ä¿å­˜å¤šç§æ¨¡å‹æ•ˆæœå›¾ï¼š
    - å•æ—¥å¯¹æ¯”ï¼ˆä½¿ç”¨å·²æœ‰ plot_prediction_dayï¼‰
    - éªŒè¯é›†æ•£ç‚¹å›¾ï¼ˆçœŸå® vs é¢„æµ‹ï¼‰
    - æ®‹å·®ç›´æ–¹å›¾
    - æ®‹å·®éšæ—¶é—´åºåˆ—å›¾
    """
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        sns.set(style='whitegrid')

        # è½¬æ¢ä¸ºnumpy
        y_true = np.array(y_val)
        y_pred = np.array(y_pred_val)
        resid = y_true - y_pred

        # 1) å•æ—¥å¯¹æ¯”å›¾
        try:
            plot_prediction_day(ts, dates_val, y_val, y_pred_val, out_dir, plot_date=plot_date, step_minutes=step_minutes)
        except Exception as e:
            print('å•æ—¥å¯¹æ¯”ç»˜å›¾å¤±è´¥: {}'.format(e))

        # 2) æ•£ç‚¹å›¾ï¼šçœŸå® vs é¢„æµ‹
        try:
            fig, ax = plt.subplots(figsize=(6, 6))
            ax.scatter(y_true, y_pred, alpha=0.6, s=10)
            mn = min(y_true.min(), y_pred.min())
            mx = max(y_true.max(), y_pred.max())
            ax.plot([mn, mx], [mn, mx], 'r--')
            if HAS_CJK_FONT:
                ax.set_xlabel('çœŸå®')
                ax.set_ylabel('é¢„æµ‹')
                ax.set_title('çœŸå® vs é¢„æµ‹ (éªŒè¯é›†)')
            else:
                ax.set_xlabel('True')
                ax.set_ylabel('Pred')
                ax.set_title('True vs Pred (val)')
            fig.savefig(os.path.join(out_dir, 'scatter_true_vs_pred_val.png'), dpi=200)
            plt.close(fig)
        except Exception as e:
            print('æ•£ç‚¹å›¾ç»˜åˆ¶å¤±è´¥: {}'.format(e))

        # 3) æ®‹å·®ç›´æ–¹å›¾
        try:
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.histplot(resid, bins=50, kde=True, ax=ax)
            if HAS_CJK_FONT:
                ax.set_title('æ®‹å·®åˆ†å¸ƒ (çœŸå® - é¢„æµ‹)')
                ax.set_xlabel('æ®‹å·®')
            else:
                ax.set_title('Residual distribution (true - pred)')
                ax.set_xlabel('Residual')
            fig.savefig(os.path.join(out_dir, 'residual_hist_val.png'), dpi=200)
            plt.close(fig)
        except Exception as e:
            print('æ®‹å·®ç›´æ–¹å›¾ç»˜åˆ¶å¤±è´¥: {}'.format(e))

        # 4) æ®‹å·®éšæ—¶é—´å›¾
        try:
            if dates_val:
                times = pd.to_datetime(dates_val)
                df_r = pd.DataFrame({'time': times, 'resid': resid, 'true': y_true, 'pred': y_pred})
                df_r = df_r.set_index('time')
                fig, ax = plt.subplots(figsize=(12, 4))
                ax.plot(df_r.index, df_r['resid'], marker='.', markersize=3, linestyle='-')
                if HAS_CJK_FONT:
                    ax.set_title('æ®‹å·®éšæ—¶é—´ (éªŒè¯é›†)')
                    ax.set_xlabel('æ—¶é—´')
                    ax.set_ylabel('æ®‹å·®')
                else:
                    ax.set_title('Residuals over time (val)')
                    ax.set_xlabel('Time')
                    ax.set_ylabel('Residual')
                fig.savefig(os.path.join(out_dir, 'residual_time_series_val.png'), dpi=200)
                plt.close(fig)
        except Exception as e:
            print('æ®‹å·®éšæ—¶é—´ç»˜å›¾å¤±è´¥: {}'.format(e))



        print('å¯è§†åŒ–å›¾åƒå·²ä¿å­˜åˆ°: {}'.format(out_dir))
    except Exception as e:
        print('å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {}'.format(e))


def select_mode():
    """é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼šè®­ç»ƒæˆ–é¢„æµ‹"""
    print("\nğŸ  å®¶åº­è´Ÿè·é¢„æµ‹ç³»ç»Ÿ")
    print("="*60)
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("1. è®­ç»ƒæ¨¡å¼ - è®­ç»ƒæ–°çš„LSTMæ¨¡å‹")
    print("2. é¢„æµ‹æ¨¡å¼ - ä½¿ç”¨å·²ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹")
    
    while True:
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
            if choice == '1':
                return 'train'
            elif choice == '2':
                return 'predict'
            else:
                print("âŒ è¯·è¾“å…¥ 1 æˆ– 2")
        except KeyboardInterrupt:
            print("\n\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return 'cancel'

def train_mode():
    """è®­ç»ƒæ¨¡å¼"""
    # äº¤äº’å¼é€‰æ‹©è¦è®­ç»ƒçš„æˆ·æ•°
    result = interactive_select_households()
    if len(result) == 3:
        selected_files, mode, custom_model_name = result
    else:
        selected_files, mode = result
        custom_model_name = None
    
    if not selected_files or mode in ['cancel', 'error']:
        print("âŒ æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶æˆ–æ“ä½œå–æ¶ˆ")
        return
    
    print(f"\nğŸš€ å¼€å§‹å•æˆ·LSTMè®­ç»ƒ")
    print("="*60)
    
    # å•æˆ·è¯¦ç»†åˆ†ææ¨¡å¼
    csv_path = selected_files[0]
    household_name = extract_household_name(csv_path)
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹åç§°æˆ–é»˜è®¤å®¶åº­åç§°
    model_name = custom_model_name if custom_model_name else household_name
    
    # ä¸ºå½“å‰æ¨¡å‹åˆ›å»ºä¸“ç”¨çš„ä¿å­˜ç›®å½•
    model_save_dir = os.path.join(MODEL_OUTPUT_DIR, model_name)
    analysis_save_dir = os.path.join(ANALYSIS_OUTPUT_DIR, model_name)
    os.makedirs(model_save_dir, exist_ok=True)
    os.makedirs(analysis_save_dir, exist_ok=True)
    
    print('å•æˆ·è¯¦ç»†åˆ†ææ¨¡å¼ï¼Œä½¿ç”¨æ–‡ä»¶: {}'.format(os.path.basename(csv_path)))
    print('å®¶åº­æ•°æ®: {}'.format(household_name))
    print('æ¨¡å‹åç§°: {}'.format(model_name))
    print('æ¨¡å‹ä¿å­˜ç›®å½•: {}'.format(model_save_dir))
    print('åˆ†æç»“æœä¿å­˜ç›®å½•: {}'.format(analysis_save_dir))
    
    ts = load_time_series(csv_path)
    if ts.empty:
        print('åŠ è½½æ—¶é—´åºåˆ—å¤±è´¥æˆ–æ— æ•°æ®')
        return

    feat_df = build_time_features(ts, sigma=2.0, seq_len=SEQ_LEN)
    if feat_df.empty:
        print('ç‰¹å¾æå–å¤±è´¥')
        return

    feat_df = feat_df.fillna(method='ffill').fillna(method='bfill')
    for c in feat_df.columns:
        if feat_df[c].isna().any():
            med = feat_df[c].median()
            feat_df[c].fillna(med if not np.isnan(med) else 0.0, inplace=True)

    X, y, dates = build_sequences_from_features(feat_df, seq_days=1, step_per_day=96)
    valid_idx = [i for i in range(len(y)) if (not np.isnan(y[i])) and (not np.isnan(X[i]).any())]
    X = X[valid_idx]; y = y[valid_idx]; dates = [dates[i] for i in valid_idx]

    (X_train, y_train, d_train), (X_test, y_test, d_test), (X_val, y_val, d_val) = time_order_split(X, y, dates, train_frac=0.7, test_frac=0.15)
    n_features = X_train.shape[2]
    scaler = StandardScaler()
    X_train_flat = X_train.reshape(-1, n_features)
    scaler.fit(X_train_flat)
    X_train_s = scaler.transform(X_train_flat).reshape(X_train.shape)
    X_test_s = scaler.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)
    X_val_s = scaler.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)

    model = build_lstm_model(input_shape=(X_train_s.shape[1], X_train_s.shape[2]))
    es = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†ä¿¡æ¯ï¼š")
    print(f"   â€¢ è®­ç»ƒé›†ï¼š{len(X_train)} æ ·æœ¬")
    print(f"   â€¢ æµ‹è¯•é›†ï¼š{len(X_test)} æ ·æœ¬") 
    print(f"   â€¢ éªŒè¯é›†ï¼š{len(X_val)} æ ·æœ¬")
    print(f"   â€¢ ç‰¹å¾ç»´åº¦ï¼š{n_features}")
    
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    model.fit(X_train_s, y_train, validation_data=(X_test_s, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)

    y_pred_val = model.predict(X_val_s).reshape(-1)
    y_pred_test = model.predict(X_test_s).reshape(-1)

    # ä¼°è®¡ä¸ç¡®å®šåº¦
    train_pred = model.predict(X_train_s).reshape(-1)
    resid = y_train - train_pred
    sigma_global = float(np.std(resid)) if len(resid) > 0 else 1e-6

    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    test_mape = mean_absolute_percentage_error(y_test, y_pred_test) * 100.0
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_r2 = r2_score(y_test, y_pred_test)
    
    val_mape = mean_absolute_percentage_error(y_val, y_pred_val) * 100.0 if len(y_val) > 0 else 0.0
    val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val)) if len(y_val) > 0 else 0.0
    val_r2 = r2_score(y_val, y_pred_val) if len(y_val) > 0 else 0.0

    # æ‰“å°æŒ‡æ ‡
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼š")
    print(f"   æµ‹è¯•é›† MAPE: {test_mape:.2f}%")
    print(f"   æµ‹è¯•é›† RMSE: {test_rmse:.4f}")
    print(f"   æµ‹è¯•é›† RÂ²:   {test_r2:.4f}")
    if len(y_val) > 0:
        print(f"   éªŒè¯é›† MAPE: {val_mape:.2f}%")
        print(f"   éªŒè¯é›† RMSE: {val_rmse:.4f}")
        print(f"   éªŒè¯é›† RÂ²:   {val_r2:.4f}")

    # ä½¿ç”¨æ¨¡å‹ä¸“ç”¨è¾“å‡ºç›®å½•
    out_dir = analysis_save_dir
    
    # ä¿å­˜æ¨¡å‹åˆ°ä¸“ç”¨æ¨¡å‹ç›®å½•
    try:
        model_path = os.path.join(model_save_dir, f'{model_name}_lstm_model.h5')
        model.save(model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        # ä¿å­˜scalerä»¥ä¾¿é¢„æµ‹æ—¶ä½¿ç”¨
        import pickle
        scaler_path = os.path.join(model_save_dir, f'{model_name}_scaler.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ°: {scaler_path}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    try:
        results_single = {
            'model_name': model_name,
            'household_name': household_name,
            'file': os.path.basename(csv_path),
            'test_mape_pct': float(test_mape),
            'test_rmse': float(test_rmse),
            'test_r2': float(test_r2),
            'val_mape_pct': float(val_mape),
            'val_rmse': float(val_rmse),
            'val_r2': float(val_r2),
            'val_samples': len(y_val),
            'test_samples': len(y_test),
            'train_samples': len(y_train),
            'n_features': n_features,
            'mode': mode,
            'sigma_global': float(sigma_global)
        }
        metrics_path = os.path.join(out_dir, f'{model_name}_metrics.csv')
        pd.DataFrame([results_single]).to_csv(metrics_path, index=False, encoding='utf-8-sig')
        print(f"âœ… è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒæŒ‡æ ‡ä¿å­˜å¤±è´¥: {e}")
    
    print(f"\nğŸ‰ å•æˆ·è®­ç»ƒå®Œæˆï¼")
    print(f"   æ¨¡å‹åç§°: {model_name}")
    print(f"   å®¶åº­æ•°æ®: {household_name}")
    print(f"   æ–‡ä»¶: {os.path.basename(csv_path)}")
    print(f"   æµ‹è¯•MAPE: {test_mape:.2f}%")
    print(f"   æ¨¡å‹ä¿å­˜åœ¨: {model_save_dir}")
    print(f"   åˆ†æç»“æœä¿å­˜åœ¨: {analysis_save_dir}")
    return

def select_saved_model():
    """é€‰æ‹©å·²ä¿å­˜çš„æ¨¡å‹"""
    print("\nğŸ” æŸ¥æ‰¾å·²ä¿å­˜çš„æ¨¡å‹...")
    
    # æŸ¥æ‰¾æ‰€æœ‰å·²ä¿å­˜çš„æ¨¡å‹
    model_files = []
    for root, dirs, files in os.walk(MODEL_OUTPUT_DIR):
        for file in files:
            if file.endswith('_lstm_model.h5'):
                model_files.append(os.path.join(root, file))
    
    if not model_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶")
        return None, None, None
    
    print(f"ğŸ“‹ å‘ç° {len(model_files)} ä¸ªå·²ä¿å­˜çš„æ¨¡å‹ï¼š")
    for i, model_file in enumerate(model_files, 1):
        # æå–æ¨¡å‹åç§°
        basename = os.path.basename(model_file)
        model_name = basename.replace('_lstm_model.h5', '')
        model_dir = os.path.dirname(model_file)
        parent_dir = os.path.basename(model_dir)
        print(f"  {i:2d}. {model_name} (ç›®å½•: {parent_dir})")
    
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(model_files)}): ").strip()
            idx = int(choice) - 1
            if 0 <= idx < len(model_files):
                model_path = model_files[idx]
                basename = os.path.basename(model_path)
                model_name = basename.replace('_lstm_model.h5', '')
                
                # æŸ¥æ‰¾å¯¹åº”çš„scaleræ–‡ä»¶
                model_dir = os.path.dirname(model_path)
                scaler_path = os.path.join(model_dir, f'{model_name}_scaler.pkl')
                
                if not os.path.exists(scaler_path):
                    print(f"âŒ æœªæ‰¾åˆ°å¯¹åº”çš„æ ‡å‡†åŒ–å™¨æ–‡ä»¶: {scaler_path}")
                    continue
                
                # éœ€è¦ç”¨æˆ·æŒ‡å®šå¯¹åº”çš„æ•°æ®æ–‡ä»¶ï¼Œå› ä¸ºæ¨¡å‹åç§°å¯èƒ½ä¸æ•°æ®æ–‡ä»¶åä¸åŒ¹é…
                print(f"\né€‰æ‹©çš„æ¨¡å‹: {model_name}")
                print("è¯·é€‰æ‹©å¯¹åº”çš„æ•°æ®æ–‡ä»¶...")
                
                # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„æ•°æ®æ–‡ä»¶
                all_data_files = find_apartment_files(DATA_FOLDER)
                print("ğŸ“‹ å¯ç”¨æ•°æ®æ–‡ä»¶ï¼š")
                for i, data_file in enumerate(all_data_files[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                    basename = os.path.basename(data_file)
                    print(f"  {i:2d}. {basename}")
                if len(all_data_files) > 10:
                    print(f"  ... è¿˜æœ‰ {len(all_data_files)-10} ä¸ªæ–‡ä»¶")
                
                while True:
                    try:
                        data_choice = input(f"\nè¯·é€‰æ‹©æ•°æ®æ–‡ä»¶åºå· (1-{len(all_data_files)}): ").strip()
                        data_idx = int(data_choice) - 1
                        if 0 <= data_idx < len(all_data_files):
                            data_file = all_data_files[data_idx]
                            print(f"âœ… é€‰æ‹©æ¨¡å‹: {model_name}")
                            print(f"âœ… é€‰æ‹©æ•°æ®: {os.path.basename(data_file)}")
                            return model_path, scaler_path, data_file
                        else:
                            print(f"âŒ è¯·è¾“å…¥ 1-{len(all_data_files)} ä¹‹é—´çš„æ•°å­—")
                    except ValueError:
                        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                    except KeyboardInterrupt:
                        print("\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                        break
            else:
                print(f"âŒ è¯·è¾“å…¥ 1-{len(model_files)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None, None, None

def select_prediction_date(ts):
    """é€‰æ‹©è¦é¢„æµ‹çš„æ—¥æœŸ"""
    print(f"\nğŸ“… æ•°æ®æ—¶é—´èŒƒå›´ï¼š")
    print(f"   å¼€å§‹æ—¶é—´: {ts.index[0]}")
    print(f"   ç»“æŸæ—¶é—´: {ts.index[-1]}")
    
    # è®¡ç®—å¯ç”¨çš„é¢„æµ‹æ—¥æœŸèŒƒå›´ï¼ˆéœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
    seq_len = SEQ_LEN * 96  # 14å¤© * 96ä¸ªç‚¹/å¤©
    available_start = ts.index[seq_len] if len(ts.index) > seq_len else ts.index[0]
    available_end = ts.index[-96] if len(ts.index) > 96 else ts.index[-1]
    
    print(f"   å¯é¢„æµ‹èŒƒå›´: {available_start.date()} è‡³ {available_end.date()}")
    
    while True:
        try:
            date_str = input("\nè¯·è¾“å…¥è¦é¢„æµ‹çš„æ—¥æœŸ (YYYY-MM-DD): ").strip()
            target_date = pd.to_datetime(date_str)
            
            if target_date < available_start:
                print(f"âŒ é¢„æµ‹æ—¥æœŸå¤ªæ—©ï¼Œéœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ï¼ˆè‡³å°‘{SEQ_LEN}å¤©ï¼‰")
                continue
            if target_date > available_end:
                print(f"âŒ é¢„æµ‹æ—¥æœŸå¤ªæ™šï¼Œæ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºé¢„æµ‹")
                continue
                
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ—¶é—´ç‚¹
            closest_idx = ts.index.get_indexer([target_date], method='nearest')[0]
            actual_date = ts.index[closest_idx]
            
            print(f"âœ… é¢„æµ‹æ—¥æœŸ: {actual_date.date()}")
            return actual_date
            
        except ValueError:
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        except KeyboardInterrupt:
            print("\n\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None

def predict_single_day(model, scaler, feat_df, target_date, step_per_day=96):
    """é¢„æµ‹å•å¤©çš„è´Ÿè· - é¢„æµ‹ç›®æ ‡æ—¥æœŸä¸€æ•´å¤©96ä¸ªæ—¶é—´ç‚¹çš„è´Ÿè·å€¼"""
    # æ³¨æ„ï¼šè¿™é‡Œçš„åºåˆ—é•¿åº¦åº”è¯¥ä¸è®­ç»ƒæ—¶ä¸€è‡´
    # è®­ç»ƒæ—¶ä½¿ç”¨çš„æ˜¯ seq_days=1, step_per_day=96ï¼Œæ‰€ä»¥åºåˆ—é•¿åº¦æ˜¯96
    seq_len = step_per_day  # 96ä¸ªæ—¶é—´æ­¥ï¼Œå¯¹åº”1å¤©çš„96ä¸ª15åˆ†é’Ÿé—´éš”
    
    # æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„å¼€å§‹ç´¢å¼•
    target_date_start = pd.Timestamp(target_date.date())
    
    # ç”Ÿæˆç›®æ ‡æ—¥æœŸçš„æ‰€æœ‰æ—¶é—´ç‚¹ï¼ˆ96ä¸ª15åˆ†é’Ÿé—´éš”ï¼‰
    target_times = pd.date_range(start=target_date_start, periods=step_per_day, freq='15T')
    
    predictions = []
    actual_times = []
    
    # æå–åºåˆ—ç‰¹å¾ï¼ˆæ’é™¤targetåˆ—ï¼‰
    feat_cols = [c for c in feat_df.columns if c != 'target_next']
    n_features = len(feat_cols)
    
    for i, target_time in enumerate(target_times):
        try:
            # æ‰¾åˆ°ç›®æ ‡æ—¶é—´åœ¨ç‰¹å¾æ•°æ®ä¸­çš„ç´¢å¼•
            target_idx = feat_df.index.get_indexer([target_time], method='nearest')[0]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
            if target_idx < seq_len:
                continue
            
            # æå–åºåˆ—ç‰¹å¾
            seq_data = feat_df.iloc[target_idx-seq_len:target_idx][feat_cols].values
            
            # æ£€æŸ¥åºåˆ—æ•°æ®çš„å½¢çŠ¶
            if seq_data.shape[0] != seq_len:
                continue
            
            # é‡å¡‘æ•°æ®ä¸ºæ­£ç¡®çš„å½¢çŠ¶ï¼š(seq_len, n_features)
            seq_reshaped = seq_data.reshape(seq_len, n_features)
            
            # æ ‡å‡†åŒ– - é€ä¸ªæ—¶é—´æ­¥æ ‡å‡†åŒ–
            seq_scaled = scaler.transform(seq_reshaped)
            
            # é‡å¡‘ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ï¼š(batch_size=1, seq_len, n_features)
            seq_scaled = seq_scaled.reshape(1, seq_len, n_features)
            
            # é¢„æµ‹
            prediction = model.predict(seq_scaled, verbose=0)[0, 0]
            
            predictions.append(float(prediction))
            actual_times.append(target_time)
            
        except Exception as e:
            print(f"é¢„æµ‹æ—¶é—´ç‚¹ {target_time} å¤±è´¥: {e}")
            continue
    
    return predictions, actual_times

def plot_single_day_prediction(ts, feat_df, pred_date, pred_values, pred_times, out_dir, step_minutes=15):
    """ç»˜åˆ¶å•å¤©é¢„æµ‹ç»“æœï¼ŒåŒ…å«HMMæ™ºèƒ½é˜¶æ®µåˆ’åˆ†"""
    try:
        day_len = int(24 * 60 / step_minutes)
        pred_date_obj = pred_date.date()
        
        # è·å–å½“å¤©çš„å®é™…æ•°æ®
        day_series = ts[ts.index.date == pred_date_obj]
        if day_series.empty:
            print(f"âŒ æœªæ‰¾åˆ°æ—¥æœŸ {pred_date_obj} çš„å®é™…æ•°æ®")
            return None
        
        # ç”Ÿæˆå½“å¤©çš„æ—¶é—´ç´¢å¼•
        base = pd.Timestamp(pred_date_obj)
        day_index = pd.date_range(start=base, periods=day_len, freq=f"{step_minutes}T")
        
        # é‡é‡‡æ ·å®é™…æ•°æ®åˆ°æ ‡å‡†æ—¶é—´ç‚¹
        if len(day_series) != day_len:
            day_series_resampled = day_series['load'].reindex(day_index, method='nearest')
        else:
            day_series_resampled = day_series['load']
        
        # å¹³æ»‘å¤„ç†
        smooth_vals = gaussian_filter1d(day_series_resampled.values.astype(float), sigma=2.0)
        
        # å¤„ç†é¢„æµ‹æ•°æ® - å°†é¢„æµ‹å€¼å¯¹é½åˆ°æ ‡å‡†æ—¶é—´ç‚¹
        pred_series = pd.Series(index=pred_times, data=pred_values)
        pred_resampled = pred_series.reindex(day_index, method='nearest').fillna(method='ffill').fillna(method='bfill')
        
        # ä½¿ç”¨HMMå¯¹é¢„æµ‹è´Ÿè·è¿›è¡Œæ™ºèƒ½é˜¶æ®µåˆ’åˆ†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è¿‡åº¦åˆ†æ®µï¼‰
        print("ğŸ”„ æ­£åœ¨è¿›è¡ŒHMMè´Ÿè·é˜¶æ®µåˆ’åˆ†...")
        try:
            # åŠ¨æ€è®¡ç®—æœ€å°æ®µé•¿åº¦ï¼šä¸€å¤©åˆ†ä¸º3-8æ®µï¼Œæ¯æ®µè‡³å°‘1.5-2å°æ—¶
            min_segment_length = max(6, len(pred_resampled) // 8)  # è‡³å°‘6ä¸ªç‚¹ï¼ˆ1.5å°æ—¶ï¼‰
            states, state_means, segments = hmm_load_segmentation(
                pred_resampled.values, 
                n_states='auto', 
                min_states=3, 
                max_states=5,  # å‡å°‘æœ€å¤§çŠ¶æ€æ•°
                min_segment_length=min_segment_length
            )
            print(f"âœ… HMMåˆ’åˆ†å®Œæˆï¼šè¯†åˆ«å‡º {len(segments)} ä¸ªè´Ÿè·é˜¶æ®µ")
            
            # æ‰“å°é˜¶æ®µä¿¡æ¯
            for i, (start_idx, end_idx, state, mean_load) in enumerate(segments):
                start_time = start_idx * step_minutes / 60
                end_time = (end_idx + 1) * step_minutes / 60
                print(f"   é˜¶æ®µ {i+1}: {start_time:05.2f}h-{end_time:05.2f}h, çŠ¶æ€={state}, å¹³å‡è´Ÿè·={mean_load:.3f}")
        except Exception as e:
            print(f"âš ï¸ HMMåˆ’åˆ†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†æ®µ: {e}")
            states, state_means, segments = simple_load_segmentation(pred_resampled.values, n_segments=4)
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(14, 8))
        time_axis = np.arange(day_len)
        
        # è½¬æ¢æ—¶é—´è½´ä¸ºå°æ—¶
        time_axis_hours = time_axis * step_minutes / 60  # è½¬æ¢ä¸ºå°æ—¶
        
        # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ - ä¸ºä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒé¢œè‰²
        colors = plt.cm.Set3(np.linspace(0, 1, len(segments)))
        
        # ç»˜åˆ¶é˜¶æ®µèƒŒæ™¯å¡«å……
        for i, (start_idx, end_idx, state, mean_load) in enumerate(segments):
            start_hour = start_idx * step_minutes / 60
            end_hour = (end_idx + 1) * step_minutes / 60
            
            # å¡«å……èƒŒæ™¯åŒºåŸŸ
            ax.axvspan(start_hour, end_hour, alpha=0.3, color=colors[i], 
                      label=f'é˜¶æ®µ{i+1} (çŠ¶æ€{state})' if HAS_CJK_FONT else f'Stage{i+1} (State{state})')
            
            # ç»˜åˆ¶é˜¶æ®µå¹³å‡è´Ÿè·çš„æ°´å¹³çº¿
            ax.hlines(mean_load, start_hour, end_hour, colors=colors[i], 
                     linestyles='--', linewidth=3, alpha=0.8)
            
            # æ·»åŠ é˜¶æ®µæ ‡æ³¨
            mid_hour = (start_hour + end_hour) / 2
            ax.text(mid_hour, mean_load + 0.1, f'é˜¶æ®µ{i+1}\n{mean_load:.2f}' if HAS_CJK_FONT else f'Stage{i+1}\n{mean_load:.2f}', 
                   ha='center', va='bottom', fontsize=8, 
                   bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.7))
        
        # ç»˜åˆ¶å®é™…è´Ÿè·æ›²çº¿
        ax.plot(time_axis_hours, smooth_vals, label='å®é™…è´Ÿè·' if HAS_CJK_FONT else 'Actual Load', 
                color='blue', linewidth=2.5, alpha=0.9, marker='o', markersize=1, zorder=3)
        
        # ç»˜åˆ¶é¢„æµ‹è´Ÿè·æ›²çº¿
        ax.plot(time_axis_hours, pred_resampled.values, label='é¢„æµ‹è´Ÿè·' if HAS_CJK_FONT else 'Predicted Load',
                color='red', linewidth=2.5, alpha=0.9, marker='x', markersize=1, zorder=3)
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlim(0, 24)
        ax.set_xticks(np.arange(0, 25, 2))
        ax.set_ylim(0, 4)
        ax.set_yticks(np.arange(0, 4.5, 0.5))
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        if HAS_CJK_FONT:
            ax.set_title(f'æ™ºèƒ½è´Ÿè·é¢„æµ‹ä¸é˜¶æ®µåˆ’åˆ† - {pred_date_obj}', fontsize=14, fontweight='bold')
            ax.set_xlabel('æ—¶é—´ (å°æ—¶)', fontsize=12)
            ax.set_ylabel('è´Ÿè·', fontsize=12)
        else:
            ax.set_title(f'Smart Load Prediction & Stage Segmentation - {pred_date_obj}', fontsize=14, fontweight='bold')
            ax.set_xlabel('Time (Hours)', fontsize=12)
            ax.set_ylabel('Load', fontsize=12)
        
        # è®¾ç½®å›¾ä¾‹
        handles, labels = ax.get_legend_handles_labels()
        # å°†é˜¶æ®µå›¾ä¾‹å’Œæ›²çº¿å›¾ä¾‹åˆ†å¼€
        stage_handles = handles[:-2]  # é˜¶æ®µå¡«å……
        curve_handles = handles[-2:]  # æ›²çº¿
        
        # åˆ›å»ºä¸¤ä¸ªå›¾ä¾‹
        legend1 = ax.legend(curve_handles, labels[-2:], loc='upper left', fontsize=10)
        ax.add_artist(legend1)  # æ·»åŠ ç¬¬ä¸€ä¸ªå›¾ä¾‹
        
        if stage_handles:  # å¦‚æœæœ‰é˜¶æ®µ
            legend2 = ax.legend(stage_handles, labels[:-2], loc='upper right', fontsize=8, ncol=2)
        
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        out_file = os.path.join(out_dir, f'prediction_with_stages_{pred_date_obj.strftime("%Y%m%d")}.png')
        fig.savefig(out_file, dpi=200, bbox_inches='tight')
        plt.close(fig)
        
        # è®¡ç®—å½“å¤©çš„ç»Ÿè®¡ä¿¡æ¯
        actual_mean = np.mean(smooth_vals)
        pred_mean = np.mean(pred_resampled.values)
        error = abs(pred_mean - actual_mean)
        mape = (error / actual_mean) * 100 if actual_mean != 0 else 0
        
        print(f"âœ… å·²ä¿å­˜é˜¶æ®µåˆ’åˆ†é¢„æµ‹å›¾: {out_file}")
        print(f"   å®é™…æ—¥å‡è´Ÿè·: {actual_mean:.4f}")
        print(f"   é¢„æµ‹æ—¥å‡è´Ÿè·: {pred_mean:.4f}")
        print(f"   ç»å¯¹è¯¯å·®: {error:.4f}")
        print(f"   MAPE: {mape:.2f}%")
        
        # ç”Ÿæˆè´Ÿè·å˜åŒ–å¯è§£é‡Šæ€§åˆ†æ
        print("\nğŸ” ç”Ÿæˆè´Ÿè·å˜åŒ–å¯è§£é‡Šæ€§åˆ†æ...")
        explanations = explain_load_changes(segments, feat_df, pred_times, pred_resampled.values)
        
        # ä¿å­˜å¯è§£é‡Šæ€§æŠ¥å‘Š
        report_path = os.path.join(out_dir, f'explanation_report_{pred_date_obj.strftime("%Y%m%d")}.txt')
        generate_explanation_report(explanations, report_path)
        
        # ä¿å­˜å¯è§£é‡Šæ€§å¯è§†åŒ–
        viz_path = os.path.join(out_dir, f'explanation_viz_{pred_date_obj.strftime("%Y%m%d")}.png')
        visualize_explanations(explanations, viz_path)
        
        # æ‰“å°ç®€è¦è§£é‡Š
        print("\nğŸ“Š è´Ÿè·å˜åŒ–è§£é‡Šæ‘˜è¦:")
        for seg in explanations.get('segment_analysis', []):
            print(f"   é˜¶æ®µ{seg['segment_id']}: {seg['load_level']} ({seg['start_time']:.1f}h-{seg['end_time']:.1f}h)")
            if seg['key_factors']:
                print(f"      å…³é”®å› ç´ : {seg['key_factors'][0]}")
        
        return {
            'date': pred_date_obj,
            'actual_mean': actual_mean,
            'predicted_mean': pred_mean,
            'error': error,
            'mape': mape,
            'image_path': out_file,
            'predictions': pred_values,
            'pred_times': pred_times,
            'segments': segments,
            'states': states.tolist(),
            'state_means': state_means.tolist(),
            'explanations': explanations,
            'explanation_report': report_path,
            'explanation_viz': viz_path
        }
        
    except Exception as e:
        print(f"âŒ ç»˜å›¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def predict_mode():
    """é¢„æµ‹æ¨¡å¼"""
    print(f"\nğŸ”® é¢„æµ‹æ¨¡å¼")
    print("="*60)

    # é€‰æ‹©æ¨¡å‹
    model_path, scaler_path, data_path = select_saved_model()
    if not model_path:
        return

    household_name = extract_household_name(data_path)
    # ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°
    model_basename = os.path.basename(model_path)
    model_name = model_basename.replace('_lstm_model.h5', '')

    print(f"ğŸ“‚ ä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"ğŸ“‚ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {os.path.basename(data_path)}")

    # åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
    try:
        import pickle
        model = tf.keras.models.load_model(model_path)
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        print(f"âœ… æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print(f"\nğŸ“Š å¤„ç†æ•°æ®...")
    ts = load_time_series(data_path)
    if ts.empty:
        print('âŒ åŠ è½½æ—¶é—´åºåˆ—å¤±è´¥æˆ–æ— æ•°æ®')
        return

    feat_df = build_time_features(ts, sigma=2.0, seq_len=SEQ_LEN)
    if feat_df.empty:
        print('âŒ ç‰¹å¾æå–å¤±è´¥')
        return

    feat_df = feat_df.fillna(method='ffill').fillna(method='bfill')
    for c in feat_df.columns:
        if feat_df[c].isna().any():
            med = feat_df[c].median()
            feat_df[c].fillna(med if not np.isnan(med) else 0.0, inplace=True)

    # åˆ›å»ºé¢„æµ‹è¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨æ¨¡å‹åç§°ï¼‰
    prediction_dir = os.path.join(ANALYSIS_OUTPUT_DIR, model_name, "predictions")
    os.makedirs(prediction_dir, exist_ok=True)

    print(f"âœ… é¢„æµ‹ç¯å¢ƒå‡†å¤‡å®Œæˆ")
    print(f"   é¢„æµ‹ç»“æœä¿å­˜åˆ°: {prediction_dir}")

    # è¿›å…¥é¢„æµ‹å¾ªç¯
    while True:
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ”® é¢„æµ‹æ¨¡å¼ - ä½¿ç”¨æ¨¡å‹: {model_name}")
            print(f"{'='*60}")

            # é€‰æ‹©é¢„æµ‹æ—¥æœŸ
            target_datetime = select_prediction_date(ts)
            if not target_datetime:
                print("âŒ æœªé€‰æ‹©æœ‰æ•ˆæ—¥æœŸ")
                continue

            print(f"\nğŸ¯ å¼€å§‹é¢„æµ‹æŒ‡å®šæ—¥æœŸ...")
            print(f"   é¢„æµ‹æ—¥æœŸ: {target_datetime.date()}")

            # å•æ—¥é¢„æµ‹
            step_per_day = 96

            try:
                # é¢„æµ‹æ•´å¤©çš„è´Ÿè·å€¼
                pred_values, pred_times = predict_single_day(model, scaler, feat_df, target_datetime, step_per_day)

                if not pred_values:
                    print(f"âŒ æ— æ³•ä¸ºæ—¥æœŸ {target_datetime.date()} ç”Ÿæˆé¢„æµ‹å€¼")
                    continue

                print(f"âœ… æˆåŠŸé¢„æµ‹ {len(pred_values)} ä¸ªæ—¶é—´ç‚¹çš„è´Ÿè·å€¼")

                # ç»˜å›¾å¹¶ä¿å­˜
                result = plot_single_day_prediction(ts, feat_df, target_datetime, pred_values, pred_times, prediction_dir)

                if result:
                    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
                    detailed_results = []
                    for i, (pred_val, pred_time) in enumerate(zip(pred_values, pred_times)):
                        detailed_results.append({
                            'date': target_datetime.date(),
                            'time': pred_time.strftime('%H:%M:%S'),
                            'predicted_load': pred_val,
                            'time_index': i
                        })

                    # ä¿å­˜è¯¦ç»†é¢„æµ‹æ•°æ®
                    detail_path = os.path.join(prediction_dir, f"prediction_detail_{target_datetime.date().strftime('%Y%m%d')}.csv")
                    pd.DataFrame(detailed_results).to_csv(detail_path, index=False, encoding='utf-8-sig')

                    # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆæ’é™¤ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
                    result_for_save = {
                        'date': result['date'],
                        'actual_mean': result['actual_mean'],
                        'predicted_mean': result['predicted_mean'],
                        'error': result['error'],
                        'mape': result['mape'],
                        'image_path': result['image_path'],
                        'num_segments': len(result.get('segments', [])),
                        'explanation_report': result.get('explanation_report', ''),
                        'explanation_viz': result.get('explanation_viz', '')
                    }
                    result_path = os.path.join(prediction_dir, f"prediction_summary_{target_datetime.date().strftime('%Y%m%d')}.csv")
                    pd.DataFrame([result_for_save]).to_csv(result_path, index=False, encoding='utf-8-sig')

                    print(f"âœ… è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {detail_path}")
                    print(f"âœ… æ±‡æ€»é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {result_path}")
                    
                    # ä¿å­˜å¯è§£é‡Šæ€§åˆ†æç»“æœï¼ˆJSONæ ¼å¼ï¼‰
                    if 'explanations' in result:
                        import json
                        explanation_json_path = os.path.join(prediction_dir, f"explanation_{target_datetime.date().strftime('%Y%m%d')}.json")
                        with open(explanation_json_path, 'w', encoding='utf-8') as f:
                            json.dump(result['explanations'], f, ensure_ascii=False, indent=2)
                        print(f"âœ… å¯è§£é‡Šæ€§åˆ†æ(JSON)å·²ä¿å­˜åˆ°: {explanation_json_path}")

                print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼")

            except Exception as e:
                print(f"âŒ é¢„æµ‹æ—¥æœŸ {target_datetime.date()} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

            # è¯¢é—®æ˜¯å¦ç»§ç»­é¢„æµ‹
            print(f"\n{'='*50}")
            while True:
                try:
                    choice = input("æ˜¯å¦ç»§ç»­é¢„æµ‹å…¶ä»–æ—¥æœŸï¼Ÿ(y/n): ").strip().lower()
                    if choice in ['y', 'yes', 'æ˜¯', '']:
                        break  # ç»§ç»­å¤–å±‚å¾ªç¯
                    elif choice in ['n', 'no', 'å¦']:
                        print(f"\nğŸ‰ é¢„æµ‹æ¨¡å¼ç»“æŸï¼æ„Ÿè°¢ä½¿ç”¨ï¼")
                        return
                    else:
                        print("âŒ è¯·è¾“å…¥ y æˆ– n")
                except KeyboardInterrupt:
                    print(f"\n\nğŸ‰ é¢„æµ‹æ¨¡å¼ç»“æŸï¼æ„Ÿè°¢ä½¿ç”¨ï¼")
                    return

        except KeyboardInterrupt:
            print(f"\n\nğŸ‰ é¢„æµ‹æ¨¡å¼ç»“æŸï¼æ„Ÿè°¢ä½¿ç”¨ï¼")
            return
        except Exception as e:
            print(f"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            # ç»§ç»­å¾ªç¯ï¼Œä¸é€€å‡ºç¨‹åº

def main():
    """ä¸»å‡½æ•°"""
    mode = select_mode()

    if mode == 'train':
        train_mode()
    elif mode == 'predict':
        predict_mode()
    elif mode == 'cancel':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
    else:
        print("âŒ æœªçŸ¥æ¨¡å¼")

if __name__ == '__main__':
    main()

