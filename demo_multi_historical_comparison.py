"""
å¤šå†å²æ—¶æœŸè´Ÿè·å¯¹æ¯”æ¼”ç¤º
å±•ç¤ºå¦‚ä½•å°†é¢„æµ‹æ—¥è´Ÿè·ä¸7/3/1å¤©å‰çš„å†å²è´Ÿè·è¿›è¡Œå¯¹æ¯”åˆ†æ

åŠŸèƒ½æ¼”ç¤º:
1. å¯¹æ¯”é¢„æµ‹æ—¥ä¸7/3/1å¤©å‰çš„é˜¶æ®µæ•°é‡å˜åŒ–
2. åˆ†æé˜¶æ®µçš„æ—¶é—´åç§»ï¼ˆå·¦ç§»/å³ç§»ï¼‰
3. åˆ†æè´Ÿè·æ°´å¹³çš„å¢å‡
4. ç»“åˆäººçš„è¡Œä¸ºæ¨¡å¼æä¾›è§£é‡Š

ç¤ºä¾‹åœºæ™¯ï¼š
- é¢„æµ‹æ—¥ï¼š2024-01-14 (å‘¨æ—¥ - å‘¨æœ«æ¨¡å¼)
- 1å¤©å‰ï¼š2024-01-13 (å‘¨å…­ - å‘¨æœ«æ¨¡å¼)
- 3å¤©å‰ï¼š2024-01-11 (å‘¨å›› - å·¥ä½œæ—¥æ¨¡å¼)  
- 7å¤©å‰ï¼š2024-01-07 (å‘¨æ—¥ - å‘¨æœ«æ¨¡å¼)

é¢„æœŸç»“æœï¼š
- ä¸3å¤©å‰(å·¥ä½œæ—¥)ç›¸æ¯”ï¼Œæ—©é«˜å³°å³ç§»(æ¨è¿Ÿ)çº¦2å°æ—¶ï¼Œå› ä¸ºå‘¨æœ«èµ·åºŠæ—¶é—´æ™š
- ä¸3å¤©å‰ç›¸æ¯”ï¼Œç™½å¤©è´Ÿè·å¢åŠ ï¼Œå› ä¸ºå‘¨æœ«åœ¨å®¶æ—¶é—´é•¿
- ä¸1å¤©å‰å’Œ7å¤©å‰(å‘¨æœ«)ç›¸æ¯”ï¼Œæ¨¡å¼ç›¸ä¼¼ä½†ç•¥æœ‰æ³¢åŠ¨
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import sys
import os

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def generate_load_data(date_str, scenario='weekday', seed=None):
    """ç”Ÿæˆè´Ÿè·æ•°æ®"""
    if seed is not None:
        np.random.seed(seed)
    
    n_points = 96  # 24å°æ—¶ * 4ä¸ª15åˆ†é’Ÿ
    times = pd.date_range(date_str, periods=n_points, freq='15min')
    hours = np.arange(n_points) / 4
    load = np.zeros(n_points)
    
    # æ ¹æ®åœºæ™¯ç”Ÿæˆä¸åŒçš„è´Ÿè·æ¨¡å¼
    for i, h in enumerate(hours):
        if scenario == 'weekday':
            # å·¥ä½œæ—¥æ¨¡å¼ï¼šæ—©é«˜å³°æ—©ï¼Œç™½å¤©è´Ÿè·ä½ï¼ˆå¤–å‡ºå·¥ä½œï¼‰
            if h < 6:
                base = 0.5
            elif h < 8:  # æ—©é«˜å³° 6-8h
                progress = (h - 6) / 2
                base = 0.5 + progress * 2.0
            elif h < 9:
                base = 2.5
            elif h < 18:  # ç™½å¤©å¤–å‡º
                base = 0.8
            elif h < 22:  # æ™šé«˜å³°
                progress = (h - 18) / 4
                base = 0.8 + progress * 2.5
            else:
                progress = (h - 22) / 2
                base = 3.3 - progress * 2.5
        
        elif scenario == 'weekend':
            # å‘¨æœ«æ¨¡å¼ï¼šæ—©é«˜å³°æ™šï¼ˆèµ·åºŠæ™šï¼‰ï¼Œç™½å¤©è´Ÿè·é«˜ï¼ˆåœ¨å®¶ï¼‰
            if h < 8:  # ç¡å¾—æ™š
                base = 0.5
            elif h < 10:  # æ—©é«˜å³°æ¨è¿Ÿåˆ° 8-10h
                progress = (h - 8) / 2
                base = 0.5 + progress * 2.5
            elif h < 11:
                base = 3.0
            elif h < 18:  # ç™½å¤©åœ¨å®¶ï¼Œè´Ÿè·è¾ƒé«˜
                base = 1.8
            elif h < 23:  # æ™šé«˜å³°ä¹Ÿæ¨è¿Ÿ
                progress = (h - 18) / 5
                base = 1.8 + progress * 2.0
            else:
                progress = (h - 23) / 1
                base = 3.8 - progress * 2.8
        
        load[i] = base + np.random.normal(0, 0.08)
    
    # ç¡®ä¿è´Ÿè·ä¸ºæ­£å€¼
    load = np.maximum(load, 0.3)
    
    # ç”Ÿæˆç¯å¢ƒç‰¹å¾
    base_temp = {'weekday': 15, 'weekend': 18}.get(scenario, 15)
    temperature = base_temp + 8 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 1, n_points)
    humidity = 60 + 12 * np.cos((hours - 3) * np.pi / 12) + np.random.normal(0, 2, n_points)
    cloudCover = np.clip(0.3 + 0.3 * np.sin(hours * np.pi / 24) + np.random.normal(0, 0.1, n_points), 0, 1)
    
    df = pd.DataFrame({
        'time': times,
        'load': load,
        'temperature_current': temperature,
        'humidity_current': humidity,
        'cloudCover_current': cloudCover,
        'hour': hours
    })
    
    df = df.set_index('time')
    return df

def simple_segmentation(load_values, n_segments=4):
    """
    ç®€å•çš„è´Ÿè·åˆ†æ®µæ–¹æ³•ï¼ˆåŸºäºåˆ†ä½æ•°ï¼‰
    å¢å¼ºç‰ˆï¼šä½¿ç”¨æ—¶é—´ç‰¹å¾çš„æ­£ä½™å¼¦ç¼–ç è®©æ—¶é—´æˆä¸ºè¿ç»­çš„ï¼Œå¹¶æ£€æµ‹è´Ÿè·å³°å€¼/æ³¢åŠ¨åŒºä½œä¸ºçª—å£åˆ’åˆ†é˜¶æ®µ
    """
    load_values = np.array(load_values)
    n = len(load_values)
    
    # æ„å»ºæ—¶é—´ç‰¹å¾ï¼ˆæ­£ä½™å¼¦ç¼–ç è®©æ—¶é—´æˆä¸ºè¿ç»­çš„ï¼‰
    features = []
    time_features = []
    for i in range(n):
        hour = (i * 0.25) % 24  # å‡è®¾15åˆ†é’Ÿé—´éš”
        time_features.append([
            np.sin(2 * np.pi * hour / 24),  # å°æ—¶çš„æ­£å¼¦ç¼–ç 
            np.cos(2 * np.pi * hour / 24),  # å°æ—¶çš„ä½™å¼¦ç¼–ç 
            np.sin(2 * np.pi * (i % 96) / 96),  # æ—¥å†…ä½ç½®ç¼–ç 
            np.cos(2 * np.pi * (i % 96) / 96)
        ])
    features.append(np.array(time_features))
    time_features = features[0]  # æå–æ—¶é—´ç‰¹å¾æ•°ç»„
    
    features.append(np.array(time_features))
    time_features = features[0]  # æå–æ—¶é—´ç‰¹å¾æ•°ç»„
    
    # æ£€æµ‹è´Ÿè·å³°å€¼/æ³¢åŠ¨åŒºåŸŸ
    from scipy.ndimage import median_filter
    smoothed_load = median_filter(load_values.astype(float), size=3)
    window_size = 8  # 2å°æ—¶çª—å£
    peak_zones = []
    fluctuation_zones = []
    
    for i in range(window_size, n - window_size):
        window = smoothed_load[i-window_size:i+window_size]
        center_val = smoothed_load[i]
        
        # å³°å€¼æ£€æµ‹ï¼šå½“å‰ç‚¹æ˜¯å±€éƒ¨æœ€å¤§å€¼
        if center_val == np.max(window) and center_val > np.percentile(smoothed_load, 75):
            peak_zones.append(i)
        
        # æ³¢åŠ¨æ£€æµ‹ï¼šçª—å£å†…æ ‡å‡†å·®è¾ƒå¤§
        window_std = np.std(window)
        if window_std > np.std(smoothed_load) * 0.8:
            fluctuation_zones.append(i)
    
    # åˆå¹¶ç›¸é‚»çš„å³°å€¼/æ³¢åŠ¨åŒºåŸŸï¼Œå½¢æˆçª—å£è¾¹ç•Œ
    def merge_zones(zones, min_gap=6):
        if not zones:
            return []
        zones = sorted(set(zones))
        merged = [zones[0]]
        for z in zones[1:]:
            if z - merged[-1] < min_gap:
                continue
            merged.append(z)
        return merged
    
    peak_boundaries = merge_zones(peak_zones)
    fluctuation_boundaries = merge_zones(fluctuation_zones)
    important_boundaries = sorted(set(peak_boundaries + fluctuation_boundaries))
    
    # å½’ä¸€åŒ–è´Ÿè·å€¼
    load_normalized = (load_values - load_values.min()) / (load_values.max() - load_values.min() + 1e-10)
    
    quantiles = np.linspace(0, 1, n_segments + 1)
    thresholds = np.quantile(load_values, quantiles)
    states = np.digitize(load_values, thresholds[1:-1])
    
    # åœ¨é‡è¦è¾¹ç•Œå¤„å¼ºåˆ¶åˆ†å‰²ï¼Œç¡®ä¿å³°å€¼/æ³¢åŠ¨åŒºä½œä¸ºç‹¬ç«‹é˜¶æ®µ
    for boundary in important_boundaries:
        if 0 < boundary < n-1:
            load_change = abs(load_normalized[boundary] - load_normalized[boundary-1])
            if load_change > 0.15:
                states[boundary] = max(0, states[boundary])
    
    # ä½¿ç”¨æ—¶é—´ç‰¹å¾ä¼˜åŒ–çŠ¶æ€è¾¹ç•Œ
    for i in range(1, n - 1):
        if states[i] != states[i-1]:
            # å¦‚æœåœ¨é‡è¦è¾¹ç•Œé™„è¿‘ï¼Œä¿æŒåˆ†å‰²
            near_boundary = any(abs(i - b) < 3 for b in important_boundaries)
            if near_boundary:
                continue
            
            # è®¡ç®—æ—¶é—´ç›¸ä¼¼åº¦
            time_sim_prev = np.dot(time_features[i], time_features[i-1])
            
            # å¦‚æœæ—¶é—´ç‰¹å¾å˜åŒ–ä¸æ˜¾è‘—ï¼Œä¸”è´Ÿè·å·®å¼‚å°ï¼Œåˆ™åˆå¹¶çŠ¶æ€
            if time_sim_prev > 0.95 and abs(load_normalized[i] - load_normalized[i-1]) < 0.1:
                states[i] = states[i-1]
    
    state_means = []
    for state in range(n_segments):
        state_mask = (states == state)
        if np.any(state_mask):
            state_means.append(np.mean(load_values[state_mask]))
        else:
            state_means.append(np.mean(load_values))
    
    state_means = np.array(state_means)
    
    segments = []
    current_state = states[0]
    start_idx = 0
    
    for i in range(1, len(states)):
        if states[i] != current_state:
            end_idx = i - 1
            segment_load = np.mean(load_values[start_idx:i])
            segments.append((start_idx, end_idx, current_state, segment_load))
            start_idx = i
            current_state = states[i]
    
    segment_load = np.mean(load_values[start_idx:])
    segments.append((start_idx, len(states) - 1, current_state, segment_load))
    
    return states, state_means, segments

print("="*80)
print("å¤šå†å²æ—¶æœŸè´Ÿè·å¯¹æ¯”åˆ†ææ¼”ç¤º")
print("="*80)
print("\nåœºæ™¯è¯´æ˜ï¼š")
print("  é¢„æµ‹æ—¥ï¼š2024-01-14 (å‘¨æ—¥ - å‘¨æœ«æ¨¡å¼)")
print("  å†å²æ•°æ®ï¼š")
print("    1å¤©å‰ (2024-01-13, å‘¨å…­ - å‘¨æœ«æ¨¡å¼)")
print("    3å¤©å‰ (2024-01-11, å‘¨å›› - å·¥ä½œæ—¥æ¨¡å¼)")
print("    7å¤©å‰ (2024-01-07, å‘¨æ—¥ - å‘¨æœ«æ¨¡å¼)")
print("="*80)

# ç”Ÿæˆæ•°æ®
print("\nâ–¶ ç”Ÿæˆé¢„æµ‹æ—¥è´Ÿè·æ•°æ® (å‘¨æœ«æ¨¡å¼)...")
predicted_df = generate_load_data('2024-01-14', scenario='weekend', seed=42)
predicted_load = predicted_df['load'].values

print("â–¶ ç”Ÿæˆå†å²è´Ÿè·æ•°æ®...")
hist_1day_df = generate_load_data('2024-01-13', scenario='weekend', seed=43)
hist_3day_df = generate_load_data('2024-01-11', scenario='weekday', seed=44)
hist_7day_df = generate_load_data('2024-01-07', scenario='weekend', seed=45)

# è¿›è¡Œè´Ÿè·é˜¶æ®µåˆ’åˆ†
print("â–¶ è¿›è¡Œè´Ÿè·é˜¶æ®µåˆ’åˆ†...")
_, _, predicted_segments = simple_segmentation(predicted_load, n_segments=5)
_, _, hist_1day_segments = simple_segmentation(hist_1day_df['load'].values, n_segments=5)
_, _, hist_3day_segments = simple_segmentation(hist_3day_df['load'].values, n_segments=5)
_, _, hist_7day_segments = simple_segmentation(hist_7day_df['load'].values, n_segments=5)

print(f"  é¢„æµ‹æ—¥é˜¶æ®µæ•°: {len(predicted_segments)}")
print(f"  1å¤©å‰é˜¶æ®µæ•°: {len(hist_1day_segments)}")
print(f"  3å¤©å‰é˜¶æ®µæ•°: {len(hist_3day_segments)}")
print(f"  7å¤©å‰é˜¶æ®µæ•°: {len(hist_7day_segments)}")

# å‡†å¤‡å¯¹æ¯”æ•°æ®
historical_data_dict = {
    1: {
        'segments': hist_1day_segments,
        'feat_df': hist_1day_df,
        'times': hist_1day_df.index.tolist(),
        'load': hist_1day_df['load'].values
    },
    3: {
        'segments': hist_3day_segments,
        'feat_df': hist_3day_df,
        'times': hist_3day_df.index.tolist(),
        'load': hist_3day_df['load'].values
    },
    7: {
        'segments': hist_7day_segments,
        'feat_df': hist_7day_df,
        'times': hist_7day_df.index.tolist(),
        'load': hist_7day_df['load'].values
    }
}

print("\nâ–¶ æ‰§è¡Œå¤šå†å²æ—¶æœŸå¯¹æ¯”åˆ†æ...")
print("  (æ­£åœ¨åŠ è½½ compare_predicted_with_multiple_historical_stages å‡½æ•°...)")

# åŠ¨æ€å¯¼å…¥å‡½æ•°
try:
    sys.path.insert(0, os.path.dirname(__file__))
    
    # ä¸´æ—¶å¤„ç†tensorflowå¯¼å…¥
    import builtins
    original_import = builtins.__import__
    
    def mock_import(name, *args, **kwargs):
        if name == 'tensorflow':
            import types
            tf = types.ModuleType('tensorflow')
            tf.random = types.ModuleType('random')
            tf.random.set_seed = lambda x: None
            return tf
        return original_import(name, *args, **kwargs)
    
    builtins.__import__ = mock_import
    
    from train_household_forecast import compare_predicted_with_multiple_historical_stages
    
    builtins.__import__ = original_import
    
    print("  âœ“ æˆåŠŸå¯¼å…¥å‡½æ•°")
    
    # æ‰§è¡Œå¤šå†å²æ—¶æœŸå¯¹æ¯”
    multi_comparison = compare_predicted_with_multiple_historical_stages(
        predicted_segments,
        historical_data_dict,
        predicted_df,
        predicted_df.index.tolist(),
        predicted_load,
        comparison_days=[1, 3, 7]
    )
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "="*80)
    print("å¯¹æ¯”åˆ†æç»“æœ")
    print("="*80)
    
    # 1. é˜¶æ®µæ•°é‡å˜åŒ–æ€»ç»“
    print("\nã€1ã€‘é˜¶æ®µæ•°é‡å˜åŒ–è¶‹åŠ¿:")
    print("-"*80)
    for sc in multi_comparison['summary']['stage_count_trends']:
        days_ago = sc['days_ago']
        change = sc['change']
        change_pct = (change / sc['historical_count'] * 100) if sc['historical_count'] > 0 else 0
        print(f"\nä¸{days_ago}å¤©å‰ç›¸æ¯”:")
        print(f"  é¢„æµ‹æ—¥é˜¶æ®µæ•°: {sc['predicted_count']}")
        print(f"  å†å²é˜¶æ®µæ•°: {sc['historical_count']}")
        print(f"  å˜åŒ–: {change:+d} ä¸ªé˜¶æ®µ ({change_pct:+.1f}%)")
    
    # 2. è¯¦ç»†çš„é€æœŸå¯¹æ¯”
    for days_ago in sorted(multi_comparison['comparisons'].keys()):
        print(f"\n{'='*80}")
        print(f"ã€2-{days_ago}ã€‘ä¸{days_ago}å¤©å‰çš„è¯¦ç»†å¯¹æ¯”")
        print("="*80)
        
        comparison = multi_comparison['comparisons'][days_ago]
        
        # æ˜¾è‘—å·®å¼‚é˜¶æ®µ
        sig_diffs = comparison.get('significant_differences', [])
        if sig_diffs:
            print(f"\nè¯†åˆ«å‡º{len(sig_diffs)}ä¸ªå·®å¼‚æ˜¾è‘—çš„é˜¶æ®µ:\n")
            
            for diff in sig_diffs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"é˜¶æ®µ {diff['current_stage']} (é¢„æµ‹æ—¥) â†” é˜¶æ®µ {diff['historical_stage']} ({days_ago}å¤©å‰):")
                print(f"  æ—¶é—´èŒƒå›´: {diff['time_range']} (é¢„æµ‹) vs {diff['historical_time_range']} (å†å²)")
                
                # æ—¶é—´åç§»
                if abs(diff['time_shift']) >= 1.0:
                    print(f"  â° æ—¶é—´åç§»: {diff['time_shift']:+.1f} å°æ—¶ ({diff['shift_direction']})")
                
                # è´Ÿè·å˜åŒ–
                if abs(diff['load_change_percent']) > 20:
                    print(f"  âš¡ è´Ÿè·å˜åŒ–: {diff['load_change']:+.2f} kW ({diff['load_change_percent']:+.1f}%)")
                
                # è¡Œä¸ºè§£é‡Š
                if diff['explanations']:
                    print(f"  ğŸ“ è§£é‡Š:")
                    for exp in diff['explanations'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                        print(f"     â€¢ {exp}")
                print()
        
        # è¡Œä¸ºæ¨¡å¼æ€»ç»“
        behavior_exps = comparison.get('behavior_explanations', [])
        if behavior_exps:
            print(f"\nè¡Œä¸ºæ¨¡å¼æ€»ç»“ (ä¸{days_ago}å¤©å‰):")
            for exp in behavior_exps[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  â€¢ {exp}")
    
    # 3. è·¨æ—¶æœŸçš„ç»¼åˆåˆ†æ
    print("\n" + "="*80)
    print("ã€3ã€‘è·¨æ—¶æœŸç»¼åˆè¡Œä¸ºæ¨¡å¼åˆ†æ")
    print("="*80)
    
    patterns = multi_comparison['summary']['behavior_patterns']
    if patterns:
        print()
        for i, pattern in enumerate(patterns, 1):
            print(f"{i}. {pattern}")
    
    # 4. æ—¶é—´åç§»è¶‹åŠ¿
    print("\nã€4ã€‘æ—¶é—´åç§»è¶‹åŠ¿:")
    print("-"*80)
    time_shifts = multi_comparison['summary']['time_shift_trends']
    if time_shifts:
        for ts in time_shifts:
            print(f"\nä¸{ts['days_ago']}å¤©å‰ç›¸æ¯”:")
            print(f"  æœ‰{ts['shift_count']}ä¸ªé˜¶æ®µå‘ç”Ÿæ—¶é—´åç§»")
            print(f"  å³ç§»(æ¨è¿Ÿ): {ts['right_shift_count']}ä¸ªé˜¶æ®µ")
            print(f"  å·¦ç§»(æå‰): {ts['left_shift_count']}ä¸ªé˜¶æ®µ")
            print(f"  ä¸»å¯¼æ–¹å‘: {ts['dominant_direction']}")
    else:
        print("  å„é˜¶æ®µæ—¶é—´åç§»ä¸æ˜¾è‘—")
    
    # 5. è´Ÿè·å˜åŒ–è¶‹åŠ¿
    print("\nã€5ã€‘è´Ÿè·å˜åŒ–è¶‹åŠ¿:")
    print("-"*80)
    load_trends = multi_comparison['summary']['load_trends']
    if load_trends:
        for lt in load_trends:
            print(f"\nä¸{lt['days_ago']}å¤©å‰ç›¸æ¯”:")
            print(f"  å·®å¼‚æ˜¾è‘—çš„é˜¶æ®µ: {lt['total_significant']}ä¸ª")
            print(f"  è´Ÿè·å¢åŠ : {lt['increase_count']}ä¸ªé˜¶æ®µ")
            print(f"  è´Ÿè·å‡å°‘: {lt['decrease_count']}ä¸ªé˜¶æ®µ")
    else:
        print("  è´Ÿè·å˜åŒ–ä¸æ˜¾è‘—")
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = '/tmp/multi_historical_comparison_report.txt'
    print(f"\nâ–¶ ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("é¢„æµ‹æ—¥è´Ÿè·ä¸å¤šå†å²æ—¶æœŸå¯¹æ¯”åˆ†ææŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write("æŠ¥å‘Šç”Ÿæˆæ—¶é—´: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
        
        f.write("åœºæ™¯è¯´æ˜ï¼š\n")
        f.write("  é¢„æµ‹æ—¥ï¼š2024-01-14 (å‘¨æ—¥ - å‘¨æœ«æ¨¡å¼)\n")
        f.write("  1å¤©å‰ï¼š2024-01-13 (å‘¨å…­ - å‘¨æœ«æ¨¡å¼)\n")
        f.write("  3å¤©å‰ï¼š2024-01-11 (å‘¨å›› - å·¥ä½œæ—¥æ¨¡å¼)\n")
        f.write("  7å¤©å‰ï¼š2024-01-07 (å‘¨æ—¥ - å‘¨æœ«æ¨¡å¼)\n\n")
        
        # å†™å…¥å®Œæ•´ç»“æœ
        for days_ago in sorted(multi_comparison['comparisons'].keys()):
            f.write(f"\n{'='*80}\n")
            f.write(f"ä¸{days_ago}å¤©å‰çš„è¯¦ç»†å¯¹æ¯”\n")
            f.write("="*80 + "\n")
            
            comparison = multi_comparison['comparisons'][days_ago]
            sig_diffs = comparison.get('significant_differences', [])
            
            if sig_diffs:
                f.write(f"\nå·®å¼‚æ˜¾è‘—çš„é˜¶æ®µ ({len(sig_diffs)}ä¸ª):\n\n")
                for diff in sig_diffs:
                    f.write(f"é˜¶æ®µ {diff['current_stage']} â†” é˜¶æ®µ {diff['historical_stage']}:\n")
                    f.write(f"  æ—¶é—´èŒƒå›´: {diff['time_range']} vs {diff['historical_time_range']}\n")
                    if abs(diff['time_shift']) >= 0.5:
                        f.write(f"  æ—¶é—´åç§»: {diff['time_shift']:+.1f} å°æ—¶\n")
                    if abs(diff['load_change_percent']) > 10:
                        f.write(f"  è´Ÿè·å˜åŒ–: {diff['load_change']:+.2f} kW ({diff['load_change_percent']:+.1f}%)\n")
                    f.write(f"  è§£é‡Š:\n")
                    for exp in diff['explanations']:
                        f.write(f"    â€¢ {exp}\n")
                    f.write("\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("ç»¼åˆè¡Œä¸ºæ¨¡å¼åˆ†æ\n")
        f.write("="*80 + "\n\n")
        for i, pattern in enumerate(patterns, 1):
            f.write(f"{i}. {pattern}\n")
    
    print("  âœ“ æŠ¥å‘Šå·²ä¿å­˜")
    
    print("\n" + "="*80)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("="*80)
    print("\nä¸»è¦å‘ç°ï¼š")
    if patterns:
        for pattern in patterns[:3]:
            print(f"  â€¢ {pattern}")
    
except Exception as e:
    print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    print("\nè¯´æ˜ï¼š")
    print("  æœ¬æ¼”ç¤ºéœ€è¦ train_household_forecast.py ä¸­çš„å‡½æ•°")
    print("  å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®")
